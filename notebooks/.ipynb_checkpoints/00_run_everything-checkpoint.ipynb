{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap: Import helpers and create directories\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to Python path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from utils.nb_helpers import run_module, run_script\n",
    "print(\"‚úÖ Notebook helpers loaded - ready to run pipeline!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PiEdge EduKit ‚Äî Guided Demo (Smoke Test)\n",
    "\n",
    "## Why this notebook exists\n",
    "A **guided demo** that runs the entire pipeline once so you can validate the environment and see the end-to-end flow before doing anything more advanced. It mirrors the mini-project \"cutlery sorter\" (Edge ML on Raspberry Pi): train a tiny classifier locally, export to ONNX, benchmark latency, attempt INT8 quantization, evaluate, and verify.\n",
    "\n",
    "## Purpose\n",
    "- Give a **practical overview** of a small Edge-ML workflow from code to measurable results.\n",
    "- Show why **ONNX** matters (same model runs on PC and Pi).\n",
    "- Teach how to read **latency metrics** (p50/p95) and why **warm-up** matters.\n",
    "- Demonstrate that **INT8 quantization may fail** on some machines and that **FP32 fallback** is acceptable in this lesson.\n",
    "\n",
    "## What you will learn\n",
    "- The pipeline: **Train ‚Üí Export (ONNX) ‚Üí Benchmark ‚Üí Quantize ‚Üí Evaluate ‚Üí Verify**.\n",
    "- How to interpret **p50/p95** and perform proper **warm-up** before timing.\n",
    "- Differences between **FP32** and **INT8** (size/latency/compatibility).\n",
    "- Where artifacts are saved and how they're used: `models/`, `reports/`, `progress/receipt.json`.\n",
    "\n",
    "## What you will produce\n",
    "- `models/model.onnx` ‚Äî exported model.\n",
    "- `reports/training_curves.png` ‚Äî training curves (visible even with 1 epoch).\n",
    "- `reports/latency_plot.png` ‚Äî latency measurement.\n",
    "- `reports/quantization_comparison.png` ‚Äî FP32 vs INT8 comparison (FP32-only if INT8 fails).\n",
    "- `reports/confusion_matrix.png` ‚Äî quick quality snapshot.\n",
    "- `progress/receipt.json` ‚Äî **receipt** with PASS/FAIL and key metrics.\n",
    "\n",
    "## Run modes\n",
    "- **Smoke Test (default, fast):** 1 epoch, few measurements ‚Üí ~2‚Äì3 min. Good for sanity check.\n",
    "- **Pretty Demo (optional):** 5 epochs, more measurements ‚Üí clearer curves & more stable stats (a few minutes extra). Provided via scripts and documented in `README.md` and `index.html`.\n",
    "\n",
    "## Prerequisites\n",
    "- **Python 3.12** inside the repo's local **`.venv`** (see README for activation).\n",
    "- Run from the **repo root** (paths are relative).\n",
    "- Everything runs on your PC. The Raspberry Pi comes later for the GPIO part.\n",
    "\n",
    "## Time budget\n",
    "- Smoke Test: ~2‚Äì3 minutes of active time.\n",
    "- Pretty Demo: ~5‚Äì7 minutes.\n",
    "\n",
    "## Success criteria\n",
    "- Notebook completes without errors.\n",
    "- Artifacts exist in `models/` and `reports/`.\n",
    "- `progress/receipt.json` shows **PASS**.\n",
    "\n",
    "> **Note:** On some Windows setups, ONNX Runtime **INT8** quantization can fail. That is **expected** here; the lesson automatically falls back to **FP32**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you run\n",
    "\n",
    "**Why this notebook exists**\n",
    "This is a *guided demo* that kicks off the full pipeline so you can verify the environment and see the end-to-end flow once.\n",
    "\n",
    "**Learning goals (quick)**\n",
    "- See the whole path once: **train ‚Üí export (ONNX) ‚Üí benchmark ‚Üí quantize ‚Üí evaluate ‚Üí verify**.\n",
    "- Know what **ONNX** is (portable inference format) and why we export to it.\n",
    "- Understand **latency metrics** (p50/p95) and why **warm-up** matters.\n",
    "- Recognize that **INT8 may fail** on some machines and that **FP32 fallback is acceptable** in this lesson.\n",
    "\n",
    "**Before you run**\n",
    "- Use **Python 3.12** inside the repo‚Äôs **`.venv`** (see README quickstart).\n",
    "- Keep the **repo root** as working directory; paths are relative.\n",
    "- Expect **quiet output** with live timers; warnings are suppressed unless relevant.\n",
    "\n",
    "**Success criteria**\n",
    "- The notebook completes without errors and generates artifacts in `models/`, `reports/`, and a **PASS** receipt in `progress/receipt.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingImports=false, reportUndefinedVariable=false, reportAttributeAccessIssue=false\n",
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd())\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"src\"))\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"sys.path configured for imports\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Run Mode Selection\n",
    "\n",
    "Choose your execution mode:\n",
    "\n",
    "**Smoke Test (default)**: Fast pipeline verification (1 epoch, 3 benchmark runs, 32 eval samples)\n",
    "- ‚úÖ Quick completion (~2-3 minutes)\n",
    "- ‚úÖ Shows 1-point training curves (with markers)\n",
    "- ‚úÖ Perfect for environment verification\n",
    "- ‚úÖ Gives PASS in verify.py\n",
    "\n",
    "**Pretty Demo**: Nice graphs for classroom (5 epochs, 200 benchmark runs, 200 eval samples)\n",
    "- üìà Clear training curves (5 points)\n",
    "- üìä Stable confusion matrix\n",
    "- ‚è±Ô∏è Takes ~5-7 minutes\n",
    "- ‚úÖ Also gives PASS in verify.py\n",
    "\n",
    "---\n",
    "\n",
    "## TODO: Run the complete pipeline\n",
    "\n",
    "This notebook demonstrates the full ML pipeline. Follow these steps:\n",
    "\n",
    "1. **Train** a model using the training script\n",
    "2. **Export** the model to ONNX format\n",
    "3. **Benchmark** inference latency\n",
    "4. **Quantize** to INT8 (may fail on some systems)\n",
    "5. **Evaluate** model performance\n",
    "6. **Verify** all artifacts are generated correctly\n",
    "\n",
    "<details><summary>Hint</summary>\n",
    "Each step generates artifacts in specific directories. Check `models/`, `reports/`, and `progress/` folders.\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "Run each cell in sequence. The pipeline will automatically generate all required artifacts and create a verification receipt.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "# 00 - Run Everything (Demo)\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "* See the complete ML pipeline from training to verification\n",
    "* Understand the purpose of each step in the workflow\n",
    "* Verify that the environment is properly configured\n",
    "\n",
    "## Concepts\n",
    "\n",
    "**Pipeline flow**: train ‚Üí export ‚Üí benchmark ‚Üí quantize ‚Üí evaluate ‚Üí verify\n",
    "\n",
    "**ONNX export**: converts PyTorch models to portable format for edge deployment\n",
    "\n",
    "**Latency benchmarking**: measures inference performance with warm-up and percentiles\n",
    "\n",
    "**Quantization**: reduces model precision (FP32 ‚Üí INT8) for faster inference\n",
    "\n",
    "**Verification**: automated checks ensure all components work correctly\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "* Running without proper Python 3.12 environment setup\n",
    "* Missing dependencies or incorrect package installation\n",
    "* File path issues when not running from repo root\n",
    "* Expecting perfect accuracy on synthetic data\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "* ‚úÖ All pipeline steps complete without errors\n",
    "* ‚úÖ Artifacts generated in correct directories\n",
    "* ‚úÖ Receipt shows PASS status\n",
    "* ‚úÖ Can explain purpose of each pipeline step\n",
    "\n",
    "## Reflection\n",
    "\n",
    "After completing this demo, reflect on:\n",
    "- Which step took the longest and why?\n",
    "- What surprised you about the pipeline flow?\n",
    "- How does this compare to other ML workflows you've seen?\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ PiEdge EduKit - Quick Run & Sanity Check\n",
    "\n",
    "## What you'll learn today\n",
    "\n",
    "* Train a tiny image classifier in PyTorch\n",
    "* Export the model to **ONNX** (a portable format for deployment)\n",
    "* Measure inference latency and interpret P50/P95\n",
    "* (Try to) quantize to INT8 and understand why it may fail\n",
    "* Evaluate the model and record a reproducible \"receipt\"\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "Most real projects train in Python but deploy elsewhere (C++, mobile, web, embedded). ONNX lets us move models **out of Python** without rewriting the model by hand.\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "This is a **smoke test**: it runs the whole pipeline end-to-end so your environment is correct. For learning and coding tasks, continue with **`01_training_and_export.ipynb`** ‚Üí **`04_evaluate_and_verify.ipynb`**.\n",
    "\n",
    "---\n",
    "\n",
    "## ONNX 101\n",
    "\n",
    "**What is ONNX?**\n",
    "ONNX (Open Neural Network Exchange) is an **open standard** for representing ML models as a graph of operators (Conv, Relu, MatMul‚Ä¶). Many frameworks can **export** to ONNX (PyTorch, TensorFlow) and many runtimes can **execute** ONNX (ONNX Runtime, TensorRT, CoreML Tools).\n",
    "\n",
    "**Why ONNX?**\n",
    "\n",
    "* **Portability**: train in Python, deploy in C++/C#/Java/JS, mobile or edge.\n",
    "* **Performance**: runtimes fuse ops and call optimized backends (MKL, cuDNN).\n",
    "* **Interoperability**: one model file can run across platforms with different \"Execution Providers\" (CPU, CUDA, DirectML, NNAPI‚Ä¶).\n",
    "\n",
    "**Key terms**\n",
    "\n",
    "* **Opset**: version of the operator set supported by runtimes. We export with a specific opset (e.g., 17).\n",
    "* **Static vs dynamic shapes**: fixed sizes are simpler/faster; dynamic adds flexibility.\n",
    "* **Execution Provider (EP)**: the backend used by ONNX Runtime (e.g., `CPUExecutionProvider`).\n",
    "* **Pre/Post-processing**: steps around the model (resize, normalize, label mapping). These **aren't** part of the ONNX graph; the app must do the same steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Mode Configuration\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create radio buttons for run mode selection\n",
    "mode_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Smoke Test (1 epoch, fast)', 'smoke'),\n",
    "        ('Pretty Demo (5 epochs, nice graphs)', 'pretty')\n",
    "    ],\n",
    "    value='smoke',\n",
    "    description='Run Mode:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Display the radio buttons\n",
    "display(mode_radio)\n",
    "\n",
    "# Set parameters based on selection\n",
    "if mode_radio.value == 'smoke':\n",
    "    EPOCHS = 1\n",
    "    BATCH_SIZE = 256\n",
    "    WARMUP_RUNS = 1\n",
    "    BENCHMARK_RUNS = 3\n",
    "    EVAL_LIMIT = 32\n",
    "    print(\"‚úÖ Smoke Test mode selected\")\n",
    "    print(\"   - Training: 1 epoch, batch-size 256\")\n",
    "    print(\"   - Benchmark: 1 warmup, 3 runs\")\n",
    "    print(\"   - Evaluation: 32 samples\")\n",
    "else:\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 16\n",
    "    WARMUP_RUNS = 50\n",
    "    BENCHMARK_RUNS = 200\n",
    "    EVAL_LIMIT = 200\n",
    "    print(\"üìà Pretty Demo mode selected\")\n",
    "    print(\"   - Training: 5 epochs, batch-size 16\")\n",
    "    print(\"   - Benchmark: 50 warmup, 200 runs\")\n",
    "    print(\"   - Evaluation: 200 samples\")\n",
    "\n",
    "print(f\"\\nParameters set: epochs={EPOCHS}, batch_size={BATCH_SIZE}, warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}, limit={EVAL_LIMIT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Verifiering\n",
    "\n",
    "First we check that the environment is correct:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiet noisy ORT quantizer log line (appears even with correct preprocessing)\n",
    "import logging\n",
    "for name in (\"\", \"onnxruntime\", \"onnxruntime.quantization\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make notebook run from repo root (not notebooks/ or labs/) + quiet mode\n",
    "import os, sys, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "def cd_repo_root():\n",
    "    p = Path.cwd()\n",
    "    for _ in range(5):  # kl√§ttra upp√•t max 5 niv√•er\n",
    "        if (p/\"verify.py\").exists() and (p/\"scripts\"/\"evaluate_onnx.py\").exists():\n",
    "            if str(p) not in sys.path: sys.path.insert(0, str(p))\n",
    "            if p != Path.cwd():\n",
    "                os.chdir(p)\n",
    "                print(\"-> Changed working dir to repo root:\", os.getcwd())\n",
    "            return\n",
    "        p = p.parent\n",
    "    raise RuntimeError(\"Could not locate repo root\")\n",
    "\n",
    "cd_repo_root()\n",
    "\n",
    "# Quiet progress bars and some noisy warnings\n",
    "os.environ.setdefault(\"TQDM_DISABLE\", \"1\")  # hide tqdm progress bars\n",
    "os.environ.setdefault(\"PYTHONWARNINGS\", \"ignore\")\n",
    "os.environ.setdefault(\"ORT_LOG_SEVERITY_LEVEL\", \"3\")  # ORT info/warn -> quiet\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"onnxruntime\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E401\n",
    "# Cross-platform runner + live clock (no shell redirection needed)\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "import shutil\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    _HAVE_WIDGETS = True\n",
    "except Exception:\n",
    "    _HAVE_WIDGETS = False\n",
    "\n",
    "@contextmanager\n",
    "def running_timer(label=\"Running‚Ä¶\"):\n",
    "    start = time.time()\n",
    "    symbols = [\"üïê\",\"üïë\",\"üïí\",\"üïì\",\"üïî\",\"üïï\",\"üïñ\",\"üïó\",\"üïò\",\"üïô\",\"üïö\",\"üïõ\"]\n",
    "    stop = False\n",
    "\n",
    "    if _HAVE_WIDGETS:\n",
    "        w = widgets.HTML()\n",
    "        display(w)\n",
    "        def _tick():\n",
    "            k = 0\n",
    "            while not stop:\n",
    "                w.value = f\"<b>{symbols[k%12]}</b> {label} &nbsp; <code>{time.time()-start:.1f}s</code>\"\n",
    "                time.sleep(0.5); k += 1\n",
    "        t = threading.Thread(target=_tick, daemon=True); t.start()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            stop = True; t.join(timeout=0.2)\n",
    "            w.value = f\"‚úÖ Done ‚Äî <code>{time.time()-start:.1f}s</code>\"\n",
    "    else:\n",
    "        width = shutil.get_terminal_size((80, 20)).columns\n",
    "        def _tick():\n",
    "            k = 0\n",
    "            while not stop:\n",
    "                msg = f\"{symbols[k%12]} {label}  {time.time()-start:.1f}s\"\n",
    "                print(\"\\r\" + msg[:width].ljust(width), end=\"\")\n",
    "                time.sleep(0.5); k += 1\n",
    "            print()\n",
    "        t = threading.Thread(target=_tick, daemon=True); t.start()\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            stop = True; t.join(timeout=0.2)\n",
    "            print(f\"‚úÖ Done ‚Äî {time.time()-start:.1f}s\")\n",
    "\n",
    "def run_module(label, module, *args):\n",
    "    \"\"\"Run `python -m <module> <args>` cross-platform, capture output, raise on error.\"\"\"\n",
    "    with running_timer(label):\n",
    "        cmd = [sys.executable, \"-W\", \"ignore\", \"-m\", module, *map(str, args)]\n",
    "        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "        print(proc.stdout)\n",
    "        if proc.returncode != 0:\n",
    "            raise RuntimeError(f\"{module} exited with code {proc.returncode}\")\n",
    "\n",
    "def run_script(label, path, *args):\n",
    "    \"\"\"Run `python <path> <args>` cross-platform, capture output, raise on error.\"\"\"\n",
    "    with running_timer(label):\n",
    "        cmd = [sys.executable, \"-W\", \"ignore\", path, *map(str, args)]\n",
    "        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "        print(proc.stdout)\n",
    "        if proc.returncode != 0:\n",
    "            raise RuntimeError(f\"{path} exited with code {proc.returncode}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check + self-healing (Python 3.12 + editable install)\n",
    "import sys, os, importlib, subprocess\n",
    "print(f\"Python version: {sys.version}\")\n",
    "assert sys.version_info[:2] == (3, 12), f\"Python 3.12 required, you have {sys.version_info[:2]}\"\n",
    "\n",
    "try:\n",
    "    import piedge_edukit  # noqa: F401\n",
    "    print(\"‚úÖ PiEdge EduKit package OK\")\n",
    "except ModuleNotFoundError:\n",
    "    # Hitta repo-roten: om vi st√•r i labs/, g√• ett steg upp\n",
    "    repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if os.path.basename(os.getcwd()) == \"labs\" else os.getcwd()\n",
    "    print(\"‚ö† Package missing ‚Äì installing editable from:\", repo_root)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", repo_root])\n",
    "    importlib.invalidate_caches()\n",
    "    import piedge_edukit  # noqa: F401\n",
    "    print(\"‚úÖ Package installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paketet ska redan vara installed av cellen ovan. Enkel sanity:\n",
    "import piedge_edukit\n",
    "print(\"‚úÖ Package imported ‚Äì continue!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Tr√§ning & ONNX Export\n",
    "\n",
    "Training a small model med FakeData och exporterar till ONNX:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (quick run for demo)\n",
    "run_module(\"Training (FakeData)\",\n",
    "           \"piedge_edukit.train\",\n",
    "           \"--fakedata\", \"--no-pretrained\",\n",
    "           \"--epochs\", 1, \"--batch-size\", 256,\n",
    "           \"--output-dir\", \"./models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the model was created\n",
    "import os\n",
    "if os.path.exists(\"./models/model.onnx\"):\n",
    "    size_mb = os.path.getsize(\"./models/model.onnx\") / (1024*1024)\n",
    "    print(f\"‚úÖ ONNX-modell skapad: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå ONNX-modell missing\")\n",
    "\n",
    "# Show training curves\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "training_plot = Path(\"reports/training_curves.png\")\n",
    "if training_plot.exists():\n",
    "    print(\"\\nüìà Training curves:\")\n",
    "    display(Image.open(training_plot))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Training curves missing ‚Äì run training first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Latency Benchmark\n",
    "\n",
    "Measuring how fast the model is on CPU:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark (quick mode)\n",
    "run_module(\"Benchmarking (CPU)\",\n",
    "           \"piedge_edukit.benchmark\",\n",
    "           \"--fakedata\",\n",
    "           \"--model-path\", \"./models/model.onnx\",\n",
    "           \"--warmup\", 1, \"--runs\", 3,\n",
    "           \"--providers\", \"CPUExecutionProvider\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visa benchmark-resultat\n",
    "if os.path.exists(\"./reports/latency_summary.txt\"):\n",
    "    with open(\"./reports/latency_summary.txt\", \"r\") as f:\n",
    "        print(\"üìä Benchmark-resultat:\")\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"‚ùå Benchmark report missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Quantization (INT8)\n",
    "\n",
    "Compressing the model for faster inference:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice: Use real training images for calibration\n",
    "# This ensures correct preprocessing and avoids ORT warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "print(\"üìä Setting up calibration data (best practice: reuse real training images)\")\n",
    "\n",
    "# Create tiny calibration image set if data/train/ is missing\n",
    "calib_dir = Path(\"data/train\")\n",
    "if not calib_dir.exists() or not any(calib_dir.rglob(\"*.png\")):\n",
    "    print(\"   Creating fallback calibration dataset...\")\n",
    "    for cls in [\"class0\", \"class1\"]:\n",
    "        (calib_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "        for i in range(16):  # 32 total (16 per class)\n",
    "            # Synthetic but \"real\" PNG files\n",
    "            arr = (np.random.rand(64, 64, 3) * 255).astype(np.uint8)\n",
    "            Image.fromarray(arr).save(calib_dir / cls / f\"sample_{i:02d}.png\")\n",
    "    print(f\"‚úÖ Created 32 fallback calibration images in {calib_dir}\")\n",
    "    print(f\"   Source: Synthetic PNG files (organized like real training data)\")\n",
    "else:\n",
    "    num_samples = sum(1 for p in calib_dir.rglob(\"*.png\"))\n",
    "    print(f\"‚úÖ Found {num_samples} existing images in {calib_dir}\")\n",
    "    print(f\"   Source: Real training data\")\n",
    "\n",
    "print(\"   ‚Üí Using --data-path ensures correct preprocessing (no ORT warning!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K√∂r kvantisering med RIKTIGA tr√§ningsbilder (korrekt preprocessing, ingen ORT-varning!)\n",
    "try:\n",
    "    if calib_dir.exists():\n",
    "        # Use real training data - same preprocessing as model was trained with\n",
    "        run_module(\"Quantization (INT8 with real calibration data)\",\n",
    "                   \"piedge_edukit.quantization\",\n",
    "                   \"--data-path\", str(calib_dir),\n",
    "                   \"--model-path\", \"./models/model.onnx\",\n",
    "                   \"--calib-size\", 32)\n",
    "    else:\n",
    "        # Fallback to FakeData (will show ORT warning)\n",
    "        run_module(\"Quantization (INT8 attempt with FakeData)\",\n",
    "                   \"piedge_edukit.quantization\",\n",
    "                   \"--fakedata\",\n",
    "                   \"--model-path\", \"./models/model.onnx\",\n",
    "                   \"--calib-size\", 16)\n",
    "except RuntimeError as e:\n",
    "    print(\"‚ö†Ô∏è Quantization step failed (OK for demo):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show quantization results\n",
    "if os.path.exists(\"./reports/quantization_summary.txt\"):\n",
    "    with open(\"./reports/quantization_summary.txt\", \"r\") as f:\n",
    "        print(\"‚ö° Kvantiseringsresultat:\")\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"‚ùå Quantization report missing\")\n",
    "\n",
    "# Tydlig notis om INT8-fail\n",
    "print(\"\\n‚ÑπÔ∏è INT8 quantization may fail on some environments. In this lesson **FP32** is accepted; verify accepts fallback.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Evaluation & Verification\n",
    "\n",
    "Testing the model and generating receipt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "from pathlib import Path\n",
    "\n",
    "run_script(\"Evaluating ONNX\",\n",
    "           str(Path(\"scripts/evaluate_onnx.py\").resolve()),\n",
    "           \"--model\", \"./models/model.onnx\",\n",
    "           \"--fakedata\", \"--limit\", 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run verification and generate receipt\n",
    "from pathlib import Path\n",
    "\n",
    "run_script(\"Verifying & generating receipt\", str(Path(\"verify.py\").resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show receipt\n",
    "import json\n",
    "if os.path.exists(\"./progress/receipt.json\"):\n",
    "    with open(\"./progress/receipt.json\", \"r\") as f:\n",
    "        receipt = json.load(f)\n",
    "    print(\"üìã Verification receipt:\")\n",
    "    print(f\"Status: {'‚úÖ PASS' if receipt['pass'] else '‚ùå FAIL'}\")\n",
    "    print(f\"Timestamp: {receipt['timestamp']}\")\n",
    "    print(\"\\nChecks:\")\n",
    "    for check in receipt['checks']:\n",
    "        status = \"‚úÖ\" if check['ok'] else \"‚ùå\"\n",
    "        print(f\"  {status} {check['name']}: {check['reason']}\")\n",
    "else:\n",
    "    print(\"‚ùå Receipt missing\")\n",
    "\n",
    "# Visa confusion matrix\n",
    "confusion_plot = Path(\"reports/confusion_matrix.png\")\n",
    "if confusion_plot.exists():\n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    display(Image.open(confusion_plot))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Confusion matrix missing ‚Äì run evaluation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "You have now completed the entire PiEdge EduKit lesson! \n",
    "\n",
    "**Next step**: Go to `01_training_and_export.ipynb` to understand what happened during training.\n",
    "\n",
    "**Generated files**:\n",
    "- `models/model.onnx` - Trained model\n",
    "- `reports/` - Benchmark and quantization reports\n",
    "- `progress/receipt.json` - Verification receipt\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Continue with detailed lessons\n",
    "\n",
    "**‚≠ê Recommended order**:\n",
    "1. **`01_training_and_export.ipynb`** - Understand training and ONNX export\n",
    "2. **`02_latency_benchmark.ipynb`** - Learn to measure performance\n",
    "3. **`03_quantization.ipynb`** - Compress models for edge\n",
    "4. **`04_evaluate_and_verify.ipynb`** - Evaluate and verify results\n",
    "\n",
    "**üí° Tip**: Each notebook builds on the previous - run them in order for best learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Glossary\n",
    "\n",
    "* **ONNX**: portable model format defined as an operator graph.\n",
    "* **Opset**: versioned set of operators supported by runtimes.\n",
    "* **Execution Provider (EP)**: backend used by ONNX Runtime (CPU, CUDA, DirectML‚Ä¶).\n",
    "* **Latency**: time per request; **Throughput**: requests per second.\n",
    "* **P50/P95/P99**: latency percentiles; tails indicate rare slow requests.\n",
    "* **Quantization (PTQ)**: convert FP32 to INT8 using calibration data.\n",
    "* **Calibration**: running representative samples to estimate activation ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (piedge)",
   "language": "python",
   "name": "piedge-edukit-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}