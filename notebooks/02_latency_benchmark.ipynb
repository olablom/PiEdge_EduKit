{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap: Import helpers and create directories\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to Python path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from utils.nb_helpers import run_module, run_script\n",
    "print(\"✅ Notebook helpers loaded - ready for benchmarking!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Latency & Throughput Benchmarking\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "* Distinguish **latency** (time for one request) vs **throughput** (requests/second).\n",
    "* Understand **warm-up**: the first inferences are slower due to lazy initialization and caches.\n",
    "* Interpret percentiles (**P50**, **P95**, **P99**) and why tail latency matters.\n",
    "* See how **batch size** trades latency for throughput.\n",
    "\n",
    "## You Should Be Able To...\n",
    "\n",
    "- Explain why warm-up runs are necessary in latency benchmarking\n",
    "- Run benchmarks with different batch sizes and interpret results\n",
    "- Calculate and compare P50/P95 latency percentiles\n",
    "- Identify performance bottlenecks in model inference\n",
    "- Make informed decisions about batch size for deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts\n",
    "\n",
    "**Warm-up runs**: prime kernels, JITs, memory. Don't include them in metrics.\n",
    "\n",
    "**P50 vs P95**: P95 tells you about \"slow outliers\". SLAs often target a percentile, not the mean.\n",
    "\n",
    "**Providers/EPs**: same ONNX model, different backends (CPU/GPU/NNAPI).\n",
    "\n",
    "**Batch size**: larger batches can improve throughput but increase per-request latency.\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "* Measuring latency without warm-up runs (first runs are slower)\n",
    "* Using mean latency instead of percentiles for SLA planning\n",
    "* Not considering batch size impact on single-request latency\n",
    "* Ignoring system variability in benchmark results\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "* ✅ Report shows mean/P50/P95 and a PNG plot\n",
    "* ✅ You can explain whether latency distribution is tight or spiky\n",
    "* ✅ You can justify a batch size for your target use case\n",
    "\n",
    "## Reflection\n",
    "\n",
    "After completing this notebook, reflect on:\n",
    "- How did batch size affect latency vs throughput?\n",
    "- Why is P95 latency more important than mean for user experience?\n",
    "- What factors contribute to latency variability?\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Environment Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E401\nimport os\nimport sys\nfrom pathlib import Path\n\nif Path.cwd().name == \"labs\":\n    os.chdir(Path.cwd().parent)\n    print(\"→ Working dir set to repo root:\", os.getcwd())\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport onnxruntime as ort\nfrom piedge_edukit.preprocess import FakeData as PEDFakeData\nimport piedge_edukit as _pkg  # noqa: F401\n\n# Hints & Solutions helper (pure Jupyter, no extra deps)\nfrom IPython.display import Markdown, display\n\ndef hints(*lines, solution: str | None = None, title=\"Need a nudge?\"):\n    \"\"\"Render progressive hints + optional collapsible solution.\"\"\"\n    md = [f\"### {title}\"]\n    for i, txt in enumerate(lines, start=1):\n        md.append(f\"<details><summary>Hint {i}</summary>\\n\\n{txt}\\n\\n</details>\")\n    if solution:\n        # keep code fenced as python for readability\n        md.append(\n            \"<details><summary><b>Show solution</b></summary>\\n\\n\"\n            f\"```python\\n{solution.strip()}\\n```\\n\"\n            \"</details>\"\n        )\n    display(Markdown(\"\\n\\n\".join(md)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment self-heal (Python 3.12 + editable install)\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]} (need 3.12)\")\n",
    "\n",
    "try:\n",
    "    import piedge_edukit  # noqa: F401\n",
    "    print(\"✅ PiEdge EduKit package OK\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"ℹ️ Installing package in editable mode …\")\n",
    "    root = os.getcwd()\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", root])\n",
    "    importlib.invalidate_caches()\n",
    "    import piedge_edukit  # noqa: F401\n",
    "    print(\"✅ Package installed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports are now in the first cell above\n",
    "print(\"✅ All imports successful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Latency vs Throughput\n",
    "\n",
    "**Latency** measures how long a single inference takes (time per prediction).\n",
    "**Throughput** measures how many inferences can be processed per second.\n",
    "\n",
    "Key metrics:\n",
    "- **Mean latency**: Average time per inference\n",
    "- **P50 latency**: Median time (50% of inferences are faster)\n",
    "- **P95 latency**: 95th percentile (95% of inferences are faster)\n",
    "- **Warm-up**: Initial runs that \"prime\" the system (GPU memory allocation, JIT compilation, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO A1 — Why warm-up?\n",
    "Write 2–3 sentences explaining graph initialization, JIT/caches and memory allocation effects on first iterations.\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "Warm-up amortizes one-time costs (kernel/JIT init, memory allocation, cache fills) so measured latency reflects steady-state. Without warm-up, p50/p95 overestimates real throughput.\n",
    "</details>\n",
    "\n",
    "hints(\n",
    "    \"The first inferences include one-off costs (graph init, memory alloc).\",\n",
    "    \"Warm-up runs stabilize timing so measured latencies reflect steady-state.\",\n",
    "    solution=\"\"\"\\\n",
    "Warm-up eliminates one-time overhead (graph compilation/init, allocator warm-up).\n",
    "It makes the reported latencies representative of steady-state performance.\"\"\"\n",
    ")\n",
    "\n",
    "## Task A: Explain Warm-up\n",
    "\n",
    "**Multiple Choice**: Why are warm-up runs important in latency benchmarking?\n",
    "\n",
    "A) They improve model accuracy\n",
    "B) They initialize system resources (GPU memory, JIT compilation, etc.)\n",
    "C) They reduce model size\n",
    "D) They increase throughput\n",
    "\n",
    "**Your answer**: _____\n",
    "\n",
    "**Short justification** (1-2 sentences): Why does this matter for accurate benchmarking?\n",
    "\n",
    "*Your answer here:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B: Batch Size Experiment\n",
    "\n",
    "Run benchmarks with different batch sizes and analyze the performance trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO B1: run latency for batch_sizes = [1, 2, 4, 8], collect p50/p95\n# Plot/print a small table and briefly interpret which batch best fits *latency-first* deployments.\n\ndef benchmark_batch_size(model_path, batch_sizes, runs=10, warmup=3):\n    \"\"\"\n    Benchmark model with different batch sizes.\n    Returns list of dicts with 'batch', 'p50', 'p95', 'mean' keys.\n    \"\"\"\n    results = []\n    \n    # Load model once\n    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n    input_name = session.get_inputs()[0].name\n    output_name = session.get_outputs()[0].name\n    \n    for batch_size in batch_sizes:\n        print(f\"Benchmarking batch size {batch_size}...\")\n        \n        # Generate test data\n        fake_data = PEDFakeData(num_samples=batch_size * runs, image_size=64, num_classes=2)\n        latencies = []\n        \n        # Warm-up runs\n        for _ in range(warmup):\n            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n            _ = session.run([output_name], {input_name: dummy_input})\n        \n        # Actual benchmark runs\n        for i in range(runs):\n            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n            \n            start_time = time.time()\n            _ = session.run([output_name], {input_name: dummy_input})\n            end_time = time.time()\n            \n            latency_ms = (end_time - start_time) * 1000\n            latencies.append(latency_ms)\n        \n        # Calculate percentiles\n        p50 = np.percentile(latencies, 50)\n        p95 = np.percentile(latencies, 95)\n        mean_lat = np.mean(latencies)\n        \n        results.append({\n            'batch': batch_size,\n            'p50': p50,\n            'p95': p95,\n            'mean': mean_lat\n        })\n        \n        print(f\"  Batch {batch_size}: P50={p50:.2f}ms, P95={p95:.2f}ms, Mean={mean_lat:.2f}ms\")\n    \n    return results\n\n# TODO: Run the experiment\n# Hint: benchmark_batch_size(\"./models/model.onnx\", [1, 8, 32], runs=10)\n\nprint(\"✅ Benchmark function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark experiment\n",
    "model_path = \"./models/model.onnx\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"❌ Model not found. Please complete Notebook 01 first.\")\n",
    "    print(\"Expected path:\", model_path)\n",
    "else:\n",
    "    # TODO: Run the benchmark with batch sizes [1, 8, 32]\n",
    "    results = benchmark_batch_size(model_path, [1, 8, 32], runs=10)\n",
    "    \n",
    "    # Display results in a table\n",
    "    print(\"\\n📊 Benchmark Results:\")\n",
    "    print(\"Batch Size | P50 (ms) | P95 (ms) | Mean (ms)\")\n",
    "    print(\"-\" * 45)\n",
    "    for r in results:\n",
    "        print(f\"{r['batch']:10} | {r['p50']:8.2f} | {r['p95']:8.2f} | {r['mean']:8.2f}\")\n",
    "    \n",
    "    # Auto-check\n",
    "    assert len(results) >= 3 and all({'batch','p50','p95'} <= set(r) for r in results)\n",
    "    print(\"✅ Results format OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hints(\n",
    "    \"Use matplotlib: hist + vertical lines at np.percentile(..., 50/95).\",\n",
    "    \"Label axes: milliseconds; title with provider/batch size.\",\n",
    "    solution='''\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "def plot_latency(lat_ms, title=\"Latency\"):\n",
    "    p50 = np.percentile(lat_ms, 50)\n",
    "    p95 = np.percentile(lat_ms, 95)\n",
    "    plt.figure()\n",
    "    plt.hist(lat_ms, bins=20)\n",
    "    plt.axvline(p50, linestyle=\"--\", label=f\"P50={p50:.2f} ms\")\n",
    "    plt.axvline(p95, linestyle=\"--\", label=f\"P95={p95:.2f} ms\")\n",
    "    plt.xlabel(\"Latency (ms)\"); plt.ylabel(\"Count\"); plt.title(title); plt.legend()\n",
    "    plt.show()\n",
    "'''\n",
    ")\n",
    "\n",
    "# Visualize the results\n",
    "if 'results' in locals() and len(results) >= 3:\n",
    "    batch_sizes = [r['batch'] for r in results]\n",
    "    p50_values = [r['p50'] for r in results]\n",
    "    p95_values = [r['p95'] for r in results]\n",
    "    mean_values = [r['mean'] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(batch_sizes, p50_values, 'o-', label='P50 Latency', linewidth=2)\n",
    "    plt.plot(batch_sizes, p95_values, 's-', label='P95 Latency', linewidth=2)\n",
    "    plt.plot(batch_sizes, mean_values, '^-', label='Mean Latency', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Latency (ms)')\n",
    "    plt.title('Latency vs Batch Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📈 Chart shows latency trends across batch sizes\")\n",
    "else:\n",
    "    print(\"⚠️ No results to visualize. Run the benchmark first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hints(\n",
    "    \"Larger batches can improve throughput but may increase single-sample latency.\",\n",
    "    \"Edge scenarios often prioritize tail latency (P95) over throughput.\",\n",
    "    solution=\"\"\"\\\n",
    "Batching amortizes overhead per call (↑ throughput), but single-request latency\n",
    "often grows with batch size. On-device UX typically targets low P95, so choose\n",
    "small batches unless you have parallel demand.\"\"\"\n",
    ")\n",
    "\n",
    "## Analysis Questions\n",
    "\n",
    "Based on your benchmark results, answer these questions:\n",
    "\n",
    "**1. How does latency change as batch size increases? Explain the trend.**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why might P95 latency be higher than P50 latency? What does this tell us about system performance?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**3. If you were deploying this model to a real-time application, which batch size would you choose and why?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Excellent work! You've learned how to measure and analyze model performance.\n",
    "\n",
    "**Next**: Open `03_quantization.ipynb` to learn about model compression and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- ✅ Understood latency vs throughput concepts\n",
    "- ✅ Implemented warm-up benchmarking\n",
    "- ✅ Analyzed batch size effects on performance\n",
    "- ✅ Interpreted P50/P95 latency metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Latency Benchmark - Understand model performance\n",
    "\n",
    "**Goal**: Understand how we measure and interpret model latency (response time).\n",
    "\n",
    "In this notebook we will:\n",
    "- Understand what latency is and why it's important\n",
    "- See how benchmark works (warmup, runs, providers)\n",
    "- Interpret results (p50, p95, histogram)\n",
    "- Experiment with different settings\n",
    "\n",
    "> **💡 Tip**: Latency is critical for edge deployment - a model that's too slow is not usable in real life!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 What is latency and why is it important?\n",
    "\n",
    "**Latency** = the time it takes for the model to make a prediction (inference time).\n",
    "\n",
    "**Why important for edge**:\n",
    "- **Real-time applications** - robots, autonomous vehicles\n",
    "- **User experience** - no one wants to wait 5 seconds for image classification\n",
    "- **Resource constraints** - Raspberry Pi has limited CPU/memory\n",
    "\n",
    "<details>\n",
    "<summary>🔍 Click to see typical latency targets</summary>\n",
    "\n",
    "**Typical latency targets**:\n",
    "- **< 10ms**: Real-time video, gaming\n",
    "- **< 100ms**: Interactive applications\n",
    "- **< 1000ms**: Batch processing, offline analysisisisisisis\n",
    "\n",
    "**Our model**: Expect ~1-10ms on CPU (good for edge!)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 How does benchmark work?\n",
    "\n",
    "**Benchmark process**:\n",
    "1. **Warmup** - run the model a few times to \"warm up\" (JIT compilation, cache)\n",
    "2. **Runs** - measure latency for many runs\n",
    "3. **Statistics** - calculate p50, p95, mean, std\n",
    "\n",
    "**Why warmup?**\n",
    "- First run is often slow (JIT compilation)\n",
    "- Cache warming affects performance\n",
    "- We want to measure \"steady state\" performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark with different settings\nprint(\"🚀 Running benchmark...\")\n\n# Use the model from the previous notebook (or create a quick one)\n!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_bench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different numbers of runs to see variance\nimport os\n\n# Test 1: Few runs (fast)\nprint(\"📊 Test 1: 10 runs\")\n!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup 3 --runs 10 --providers CPUExecutionProvider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Benchmark results\nif os.path.exists(\"./reports/latency_summary.txt\"):\n    with open(\"./reports/latency_summary.txt\", \"r\") as f:\n        print(\"📈 Benchmark results:\")\n        print(f.read())\nelse:\n    print(\"❌ Benchmark report missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read detailed latency data and visualize\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nif os.path.exists(\"./reports/latency.csv\"):\n    df = pd.read_csv(\"./reports/latency.csv\")\n    \n    print(f\"📊 Latency statistics:\")\n    print(f\"Num measurements: {len(df)}\")\n    print(f\"Mean: {df['latency_ms'].mean():.2f} ms\")\n    print(f\"Std: {df['latency_ms'].std():.2f} ms\")\n    print(f\"Min: {df['latency_ms'].min():.2f} ms\")\n    print(f\"Max: {df['latency_ms'].max():.2f} ms\")\n    \n    # Histogram\n    plt.figure(figsize=(10, 6))\n    plt.hist(df['latency_ms'], bins=20, alpha=0.7, edgecolor='black')\n    plt.xlabel('Latency (ms)')\n    plt.ylabel('Count')\n    plt.title('Latency distribution')\n    plt.grid(True, alpha=0.3)\n    plt.show()\n    \n    # Box plot\n    plt.figure(figsize=(8, 6))\n    plt.boxplot(df['latency_ms'])\n    plt.ylabel('Latency (ms)')\n    plt.title('Latency Box Plot')\n    plt.grid(True, alpha=0.3)\n    plt.show()\nelse:\n    print(\"❌ Latency CSV missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Reflection Questions\n\n<details>\n<summary>💭 Why is p95 more important than mean for edge deployment?</summary>\n\n**Answer**: p95 (95th percentile) shows the worst latency that 95% of users experience. It is more important than mean because:\n\n- **User experience**: A user who gets 100ms latency will notice it, even if the mean is 10ms\n- **SLA targets**: Many systems have SLA targets at p95 latency\n- **Outliers**: Mean can be skewed by outliers; p95 is more robust\n\n</details>\n\n<details>\n<summary>💭 What happens to latency variance when you increase the number of runs?</summary>\n\n**Answer**: With more runs we get:\n- **More stable statistics** - p50/p95 become more reliable\n- **Better understanding of variance** - see if the model is consistent\n- **Less impact of outliers** - occasional slow runs matter less\n\n**Experiment**: Run the benchmark with 10, 50, 100 runs and compare standard deviation.\n\n</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Your own experiment\n\n**Task**: Run the benchmark with different settings and compare the results.\n\n**Suggestions**:\n- Try different numbers of runs (10, 50, 100)\n- Compare the warmup effect (0, 3, 10 warmup)\n- Analyze the variance between runs\n\n**Code to modify**:\n```python\n# Change these values:\nWARMUP_RUNS = 5\nBENCHMARK_RUNS = 50\n\n!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your experiment here\n# Change the values below and run the benchmark\n\nWARMUP_RUNS = 5\nBENCHMARK_RUNS = 50\n\nprint(f\"🧪 My experiment: warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}\")\n\n# TODO: Run the benchmark with your settings\n# !python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Summary\n\nYou have now learned:\n- What latency is and why it is critical for edge deployment\n- How the benchmark works (warmup, runs, statistics)\n- How to interpret latency results (p50, p95, variance)\n- Why P95 is more important than mean for user experience\n\n**Next step**: Go to `03_quantization.ipynb` to understand how quantization can improve performance.\n\n**Key concepts**:\n- **Latency**: Inference time (critical for edge)\n- **Warm-up**: Prepares the model for measurement\n- **p50/p95**: Percentiles for the latency distribution\n- **Variance**: Consistency in performance\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}