{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap: Import helpers and create directories\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo root to Python path\n",
    "repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "from utils.nb_helpers import run_module, run_script\n",
    "print(\"‚úÖ Notebook helpers loaded - ready for evaluation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Evaluation & Verification\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "* Compute and interpret simple metrics (accuracy, confusion matrix).\n",
    "* Produce reproducible **artifacts** and a machine-readable **receipt** for CI.\n",
    "* Understand why verifiable outputs matter in a classroom or production pipeline.\n",
    "\n",
    "## You Should Be Able To...\n",
    "\n",
    "- Run model evaluation and interpret results\n",
    "- Understand confusion matrices and accuracy metrics\n",
    "- Generate verification receipts for ML pipelines\n",
    "- Identify when models meet deployment criteria\n",
    "- Reflect on the complete ML development process\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts\n",
    "\n",
    "**Confusion matrix**: where the classifier makes mistakes by class.\n",
    "\n",
    "**Reproducible artifacts**: model file, benchmark reports, quantization summary, evaluation report.\n",
    "\n",
    "**Receipt**: a small JSON proving all required files were created and basic checks passed.\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "* Not running evaluation on held-out test data\n",
    "* Misinterpreting confusion matrix results\n",
    "* Forgetting to generate verification artifacts\n",
    "* Not checking that all pipeline components work together\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "* ‚úÖ `progress/receipt.json` says **PASS**\n",
    "* ‚úÖ You can explain what each artifact is and where it lives\n",
    "* ‚úÖ You can describe one change you'd make next (e.g., more data, different architecture)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Environment Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ruff: noqa: E401\nimport os\nimport sys\nfrom pathlib import Path\n\ndef cd_repo_root():\n    p = Path.cwd()\n    for _ in range(5):  # climb up at most 5 levels\n        if (p/\"verify.py\").exists() and (p/\"scripts\"/\"evaluate_onnx.py\").exists():\n            if str(p) not in sys.path: sys.path.insert(0, str(p))\n            if p != Path.cwd():\n                os.chdir(p)\n                print(\"-> Changed working dir to repo root:\", os.getcwd())\n            return\n        p = p.parent\n    raise RuntimeError(\"Could not locate repo root\")\n\ncd_repo_root()\n\n# Hints & Solutions helper (pure Jupyter, no extra deps)\nfrom IPython.display import Markdown, display\n\ndef hints(*lines, solution: str | None = None, title=\"Need a nudge?\"):\n    \"\"\"Render progressive hints + optional collapsible solution.\"\"\"\n    md = [f\"### {title}\"]\n    for i, txt in enumerate(lines, start=1):\n        md.append(f\"<details><summary>Hint {i}</summary>\\n\\n{txt}\\n\\n</details>\")\n    if solution:\n        # keep code fenced as python for readability\n        md.append(\n            \"<details><summary><b>Show solution</b></summary>\\n\\n\"\n            f\"```python\\n{solution.strip()}\\n```\\n\"\n            \"</details>\"\n        )\n    display(Markdown(\"\\n\\n\".join(md)))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î What is evaluation and why do we need it?\n\n**Evaluation** = test the model on data it has not seen during training.\n\n**What we measure**:\n- **Accuracy** ‚Äî how many predictions are correct\n- **Confusion matrix** ‚Äî detailed breakdown of correct/incorrect predictions\n- **Per-class performance** ‚Äî how well the model performs for each class\n\n**Why important**:\n- **Validation** ‚Äî ensures the model actually works\n- **Debugging** ‚Äî shows which classes are difficult\n- **Comparison** ‚Äî compare different models/settings\n\n<details>\n<summary>üîç Click to see what a confusion matrix shows</summary>\n\n**Confusion matrix**:\n- **Diagonal** = correct predictions\n- **Off-diagonal** = incorrect predictions\n- **Per class** = precision, recall for each class\n\n</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on our model\n",
    "print(\"üîç Running evaluation...\")\n",
    "\n",
    "# Use the model from previous notebooks (or create a quick one)\n",
    "!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with a limited number of samples (faster)\n",
    "!python scripts/evaluate_onnx.py --model ./models_eval/model.onnx --fakedata --limit 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation results\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"./reports/eval_summary.txt\"):\n",
    "    with open(\"./reports/eval_summary.txt\", \"r\") as f:\n",
    "        print(\"üìä Evaluation results:\")\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"‚ùå Evaluation report missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curves if available\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "if os.path.exists(\"./reports/training_curves.png\"):\n",
    "    print(\"üìà Training curves:\")\n",
    "    display(Image.open(\"./reports/training_curves.png\"))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Training curves missing ‚Äì run training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix if it exists\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nif os.path.exists(\"./reports/confusion_matrix.png\"):\n    print(\"üìà Confusion Matrix:\")\n    img = Image.open(\"./reports/confusion_matrix.png\")\n    plt.figure(figsize=(8, 6))\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('Confusion Matrix')\n    plt.show()\nelse:\n    print(\"‚ùå Confusion matrix missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Automatic verification\n",
    "\n",
    "**Verification** = automated checks ensuring the lesson works correctly.\n",
    "\n",
    "**What is checked**:\n",
    "- **Artifacts exist** ‚Äî all required files are created\n",
    "- **Benchmark works** ‚Äî latency data is valid\n",
    "- **Quantization works** ‚Äî quantized model is created\n",
    "- **Evaluation works** ‚Äî confusion matrix and accuracy are available\n",
    "\n",
    "**Result**: `progress/receipt.json` with PASS/FAIL status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automatic verification\n",
    "print(\"üîç Running automatic verification...\")\n",
    "!python verify.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the receipt in detail\n",
    "import json\n",
    "\n",
    "if os.path.exists(\"./progress/receipt.json\"):\n",
    "    with open(\"./progress/receipt.json\", \"r\") as f:\n",
    "        receipt = json.load(f)\n",
    "    \n",
    "    print(\"üìã Detailed receipt analysis:\")\n",
    "    print(f\"Status: {'‚úÖ PASS' if receipt['pass'] else '‚ùå FAIL'}\")\n",
    "    print(f\"Timestamp: {receipt['timestamp']}\")\n",
    "    \n",
    "    print(\"\\nüîç Checks:\")\n",
    "    for check in receipt['checks']:\n",
    "        status = \"‚úÖ\" if check['ok'] else \"‚ùå\"\n",
    "        print(f\"  {status} {check['name']}: {check['reason']}\")\n",
    "    \n",
    "    print(\"\\nüìä Metrics:\")\n",
    "    if 'metrics' in receipt:\n",
    "        for metric, value in receipt['metrics'].items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    \n",
    "    print(\"\\nüìÅ Generated files:\")\n",
    "    if 'artifacts' in receipt:\n",
    "        for artifact in receipt['artifacts']:\n",
    "            print(f\"  - {artifact}\")\n",
    "else:\n",
    "    print(\"‚ùå Receipt missing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Reflection Questions\n\n### TODO R1 ‚Äî Reflect on results (2‚Äì4 bullets)\n- Where did quantization help / hurt?\n- Do your p50 and p95 match expectations after warm-up?\n- One change you would make before deploying.\n\n<details><summary>Hint</summary>\nTie back to goals: correctness, latency, and determinism. Fallback to FP32 is fine if INT8 regresses.\n</details>\n\n<details>\n<summary>üí≠ Which goals are verified by our automatic check?</summary>\n\n**Answer**: Our verification checks:\n- **Technical functionality** ‚Äî all steps run without errors\n- **Artifact generation** ‚Äî required files are created\n- **Data integrity** ‚Äî reports are valid and parseable\n- **Pipeline integration** ‚Äî all components work together\n\n**What is NOT verified**:\n- Accuracy quality (only that evaluation runs)\n- Latency targets (only that benchmark runs)\n- Production readiness (only that the pipeline works)\n\n</details>\n\n<details>\n<summary>üí≠ What is missing for \"production\"?</summary>\n\n**Answer**: For production we need:\n- **Real data** ‚Äî not FakeData\n- **Accuracy targets** ‚Äî specific precision/recall requirements\n- **Latency targets** ‚Äî SLA requirements on inference time\n- **Robustness** ‚Äî handling of edge cases and errors\n- **Monitoring** ‚Äî continuous monitoring of performance\n- **A/B testing** ‚Äî comparison of different models\n- **Rollback** ‚Äî ability to revert to previous versions\n\n</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Your own experiment\n\n**Task**: Run verification on different models and compare receipts.\n\n**Suggestions**:\n- Train models with different settings\n- Run verification on each model\n- Compare receipts and see which pass/fail\n- Analyze which checks are most critical\n\n**Code to modify**:\n```python\n# Train different models and run verification\nMODELS = [\n    {\"epochs\": 1, \"batch_size\": 128, \"name\": \"quick\"},\n    {\"epochs\": 3, \"batch_size\": 64, \"name\": \"balanced\"},\n    {\"epochs\": 5, \"batch_size\": 32, \"name\": \"thorough\"}\n]\n\nfor model_config in MODELS:\n    # Train model\n    # Run verification\n    # Analyze the receipt\n```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your experiment here\n# Train different models and compare the receipts\n\nMODELS = [\n    {\"epochs\": 1, \"batch_size\": 128, \"name\": \"quick\"},\n    {\"epochs\": 3, \"batch_size\": 64, \"name\": \"balanced\"},\n    {\"epochs\": 5, \"batch_size\": 32, \"name\": \"thorough\"}\n]\n\nprint(\"üß™ My experiment: Compare different models\")\nfor model_config in MODELS:\n    print(f\"  - {model_config['name']}: epochs={model_config['epochs']}, batch_size={model_config['batch_size']}\")\n\n# TODO: Implement a loop that trains and verifies each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "Congratulations! You've completed the entire PiEdge EduKit lesson. Please reflect on your learning experience:\n",
    "\n",
    "**1. What was the most challenging part of implementing the CNN architecture? What helped you understand it better?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**2. How did your understanding of model performance change after running the latency benchmarks?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**3. What surprised you most about the quantization process? What would you do differently in a real deployment?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "**4. How important do you think automated verification is for ML pipelines? Why?**\n",
    "\n",
    "*Your answer here (2-3 sentences):*\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Congratulations!** You've successfully completed the PiEdge EduKit lesson. You now understand:\n",
    "\n",
    "- ‚úÖ CNN implementation and training\n",
    "- ‚úÖ Model export to ONNX format  \n",
    "- ‚úÖ Performance benchmarking and analysis\n",
    "- ‚úÖ Quantization and compression techniques\n",
    "- ‚úÖ Evaluation and verification workflows\n",
    "\n",
    "**Real-world applications**: Experiment with real data, different models, or deploy on Raspberry Pi!\n",
    "\n",
    "**Key concepts mastered**:\n",
    "- **Training**: Implementing and training neural networks\n",
    "- **Export**: Converting models to deployment-ready formats\n",
    "- **Benchmarking**: Measuring and analyzing performance\n",
    "- **Quantization**: Optimizing models for edge deployment\n",
    "- **Verification**: Automated quality assurance for ML pipelines\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}