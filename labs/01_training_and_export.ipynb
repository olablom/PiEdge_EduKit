{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Training & Export\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "- How to implement a simple CNN architecture for image classification\n",
        "- The components of a PyTorch training loop\n",
        "- How to export models to ONNX format for edge deployment\n",
        "- The difference between training and inference modes\n",
        "\n",
        "## You Should Be Able To...\n",
        "- Implement a basic CNN using PyTorch layers\n",
        "- Write a training loop with loss calculation and accuracy tracking\n",
        "- Export a trained model to ONNX format with proper input/output specifications\n",
        "- Explain why ONNX export is useful for edge deployment\n",
        "- Identify key hyperparameters that affect model performance\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Environment Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ruff: noqa: E401\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure repo root in path if opened from labs/\n",
        "if Path.cwd().name == \"labs\":\n",
        "    os.chdir(Path.cwd().parent)\n",
        "    print(\"‚Üí Working dir set to repo root:\", os.getcwd())\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Core deps\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import onnx\n",
        "from onnx import checker  # noqa: F401\n",
        "import onnxruntime as ort  # noqa: F401\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import FakeData\n",
        "\n",
        "# Project package\n",
        "from piedge_edukit.preprocess import FakeData\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment self-heal (Python 3.12 + editable install)\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "print(f\"Python: {sys.version.split()[0]} (need 3.12)\")\n",
        "\n",
        "try:\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"‚úÖ PiEdge EduKit package OK\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"‚ÑπÔ∏è Installing package in editable mode ‚Ä¶\")\n",
        "    root = os.getcwd()\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", root])\n",
        "    importlib.invalidate_caches()\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"‚úÖ Package installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All imports are now in the first cell above\n",
        "print(\"‚úÖ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept: Convolutional Neural Networks\n",
        "\n",
        "CNNs are designed to process grid-like data (images) by:\n",
        "- **Convolutional layers**: Learn spatial patterns (edges, textures, shapes)\n",
        "- **Pooling layers**: Reduce spatial dimensions while preserving important features\n",
        "- **Fully connected layers**: Make final classification decisions\n",
        "\n",
        "For 64√ó64 RGB images, a typical architecture flows: `[3,64,64] ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí Linear ‚Üí Linear ‚Üí [num_classes]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task A: Implement a Simple CNN\n",
        "\n",
        "Your task is to implement a `TinyCNN` class that can classify 64√ó64 RGB images into 2 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement a tiny CNN suitable for 64x64 images.\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int = 2):\n",
        "        super().__init__()\n",
        "        # TODO: replace `...` with layers:\n",
        "        # Hint: Conv2d -> ReLU -> MaxPool -> Conv2d -> ReLU -> MaxPool -> Flatten -> Linear -> Linear\n",
        "        self.net = nn.Sequential(\n",
        "            # TODO: Add your layers here\n",
        "            # Example structure:\n",
        "            # nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool2d(2),\n",
        "            # ... continue the pattern\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Create model instance\n",
        "model = TinyCNN(num_classes=2)\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST: model should accept [1,3,64,64] and output [1,2]\n",
        "# (torch already imported in first cell)\n",
        "x = torch.randn(1,3,64,64)\n",
        "y = model(x)\n",
        "assert y.shape == (1,2), f\"Expected (1,2), got {tuple(y.shape)}\"\n",
        "print(\"‚úÖ Shape test passed\")\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output range: [{y.min().item():.3f}, {y.max().item():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept: Training Loop Components\n",
        "\n",
        "A typical training loop includes:\n",
        "1. **Forward pass**: Compute predictions\n",
        "2. **Loss calculation**: Compare predictions to ground truth\n",
        "3. **Backward pass**: Compute gradients\n",
        "4. **Optimizer step**: Update model parameters\n",
        "5. **Metrics tracking**: Monitor loss and accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task B: Write the Training Step\n",
        "\n",
        "Implement a `train_one_epoch` function that trains the model for one epoch and returns loss and accuracy metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement train_one_epoch(model, loader, optimizer, device)\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # TODO: implement the training loop\n",
        "    # Hint: iterate through loader, move data to device, compute loss, backward, step optimizer\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        # TODO: Move data and target to device\n",
        "        # TODO: Zero gradients\n",
        "        # TODO: Forward pass\n",
        "        # TODO: Compute loss\n",
        "        # TODO: Backward pass\n",
        "        # TODO: Update parameters\n",
        "        # TODO: Track metrics (loss, accuracy)\n",
        "        pass\n",
        "    \n",
        "    return {\"loss\": loss_sum/len(loader), \"acc\": 100*correct/max(1,total)}\n",
        "\n",
        "# Test the function signature\n",
        "print(\"‚úÖ Function signature looks correct\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test data and test the training function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create fake data loader\n",
        "fake_data = FakeData(num_samples=100, image_size=64, num_classes=2)\n",
        "train_loader = DataLoader(fake_data, batch_size=16, shuffle=True)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Test training function\n",
        "metrics = train_one_epoch(model, train_loader, optimizer, device)\n",
        "assert \"loss\" in metrics and \"acc\" in metrics\n",
        "print(\"‚úÖ Training loop smoke test passed\")\n",
        "print(f\"Loss: {metrics['loss']:.4f}, Accuracy: {metrics['acc']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept: ONNX Export\n",
        "\n",
        "ONNX (Open Neural Network Exchange) is a format that allows models to run on different platforms:\n",
        "- **Cross-platform**: Same model runs on CPU, GPU, mobile, edge devices\n",
        "- **Optimized inference**: ONNX Runtime provides optimized execution\n",
        "- **Language agnostic**: Models can be used from Python, C++, C#, JavaScript, etc.\n",
        "\n",
        "Key requirements for export:\n",
        "- Model must be in evaluation mode (`model.eval()`)\n",
        "- Provide a dummy input with correct shape\n",
        "- Specify input/output names for clarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task C: Export to ONNX\n",
        "\n",
        "Export your trained model to ONNX format for edge deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: export model to ./models/student_model.onnx (opset 17)\n",
        "# Hints:\n",
        "#  - torch.onnx.export(model.eval(), dummy, path, opset_version=17, input_names=..., output_names=...)\n",
        "#  - dummy = torch.randn(1,3,64,64)\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs(\"./models\", exist_ok=True)\n",
        "\n",
        "# TODO: Create dummy input\n",
        "# TODO: Export model to ONNX\n",
        "# TODO: Set model to evaluation mode first\n",
        "\n",
        "print(\"‚úÖ ONNX export completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ONNX export\n",
        "# (onnx and os already imported in first cell)\n",
        "assert os.path.exists(\"./models/student_model.onnx\"), \"ONNX file missing\"\n",
        "m = onnx.load(\"./models/student_model.onnx\")\n",
        "onnx.checker.check_model(m)\n",
        "print(\"‚úÖ ONNX export verified\")\n",
        "\n",
        "# Show model info\n",
        "file_size = os.path.getsize(\"./models/student_model.onnx\") / (1024*1024)\n",
        "print(f\"Model size: {file_size:.2f} MB\")\n",
        "print(f\"Input shape: {[d.dim_value for d in m.graph.input[0].type.tensor_type.shape.dim]}\")\n",
        "print(f\"Output shape: {[d.dim_value for d in m.graph.output[0].type.tensor_type.shape.dim]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection Questions\n",
        "\n",
        "Please answer these questions in 2-3 sentences each:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. What two hyperparameters most affected your validation accuracy? Why?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**2. Why is exporting to ONNX useful for edge deployment?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**3. What would happen if you forgot to call `model.eval()` before ONNX export?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Great work! You've implemented a CNN, trained it, and exported it to ONNX format.\n",
        "\n",
        "**Next**: Open `02_latency_benchmark.ipynb` to learn about performance measurement and optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "- ‚úÖ Implemented TinyCNN architecture\n",
        "- ‚úÖ Created training loop with metrics\n",
        "- ‚úÖ Exported model to ONNX format\n",
        "- ‚úÖ Verified export integrity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Tr√§ning & ONNX Export - F√∂rst√• vad som h√§nder\n",
        "\n",
        "**M√•l**: F√∂rst√• hur tr√§ning fungerar och experimentera med olika inst√§llningar.\n",
        "\n",
        "I detta notebook kommer vi att:\n",
        "- F√∂rst√• vad FakeData √§r och varf√∂r vi anv√§nder det\n",
        "- Se hur dataset-pipeline ‚Üí modell ‚Üí loss/accuracy fungerar\n",
        "- Experimentera med olika hyperparametrar\n",
        "- F√∂rst√• varf√∂r vi exporterar till ONNX\n",
        "\n",
        "> **üí° Tips**: K√∂r cellerna i ordning och l√§s f√∂rklaringarna. Experimentera g√§rna med v√§rdena!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Vad √§r FakeData och varf√∂r anv√§nder vi det?\n",
        "\n",
        "**FakeData** √§r syntetiska bilder som PyTorch genererar automatiskt. Det √§r perfekt f√∂r:\n",
        "- **Snabb prototyping** - ingen nedladdning av stora dataset\n",
        "- **Reproducerbarhet** - samma data varje g√•ng\n",
        "- **Undervisning** - fokus p√• algoritmer, inte datahantering\n",
        "\n",
        "<details>\n",
        "<summary>üîç Klicka f√∂r att se vad FakeData inneh√•ller</summary>\n",
        "\n",
        "```python\n",
        "# FakeData genererar:\n",
        "# - Slumpm√§ssiga RGB-bilder (64x64 pixlar)\n",
        "# - Slumpm√§ssiga klasser (0, 1, 2, ...)\n",
        "# - Samma struktur som riktiga bilddataset\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L√•t oss skapa en liten FakeData f√∂r att se vad den inneh√•ller\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Skapa FakeData med 2 klasser\n",
        "fake_data = datasets.FakeData(size=10, num_classes=2, transform=None)\n",
        "\n",
        "# Visa f√∂rsta bilden\n",
        "image, label = fake_data[0]\n",
        "print(f\"Bildstorlek: {image.size}\")\n",
        "print(f\"Klass: {label}\")\n",
        "print(f\"Pixelv√§rden: {image.getextrema()}\")\n",
        "\n",
        "# Visa bilden\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.imshow(image)\n",
        "plt.title(f\"FakeData - Klass {label}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Experimentera med Tr√§ning\n",
        "\n",
        "Nu ska vi tr√§na en modell och se hur olika inst√§llningar p√•verkar resultatet.\n",
        "\n",
        "**Hyperparametrar att experimentera med**:\n",
        "- `epochs` - antal genomg√•ngar av datasetet\n",
        "- `batch_size` - antal bilder per tr√§ningssteg\n",
        "- `--no-pretrained` - b√∂rja fr√•n noll vs f√∂rtr√§nade vikter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 1: Snabb tr√§ning (1 epoch, ingen pretrained)\n",
        "print(\"üß™ Experiment 1: Snabb tr√§ning\")\n",
        "!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 128 --output-dir ./models_exp1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visa tr√§ningsresultat fr√•n Experiment 1\n",
        "import json\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./models_exp1/training_info.json\"):\n",
        "    with open(\"./models_exp1/training_info.json\", \"r\") as f:\n",
        "        info = json.load(f)\n",
        "    \n",
        "    print(\"üìä Tr√§ningsresultat (Experiment 1):\")\n",
        "    print(f\"Final accuracy: {info.get('final_accuracy', 'N/A'):.3f}\")\n",
        "    print(f\"Final loss: {info.get('final_loss', 'N/A'):.3f}\")\n",
        "    print(f\"Epochs: {info.get('epochs', 'N/A')}\")\n",
        "    print(f\"Batch size: {info.get('batch_size', 'N/A')}\")\n",
        "else:\n",
        "    print(\"‚ùå Tr√§ningsinfo saknas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Reflektionsfr√•gor\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ Vad h√§nder med √∂verfitting n√§r du h√∂jer epochs?</summary>\n",
        "\n",
        "**Svar**: Med fler epochs kan modellen l√§ra sig tr√§ningsdata f√∂r bra och d√•ligt generalisera till nya data. Detta kallas √∂verfitting.\n",
        "\n",
        "**Experiment**: K√∂r samma tr√§ning men med `--epochs 5` och j√§mf√∂r accuracy p√• tr√§nings- vs valideringsdata.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ Varf√∂r exporterar vi till ONNX (f√∂r Pi/edge)?</summary>\n",
        "\n",
        "**Svar**: ONNX √§r ett standardformat som fungerar p√• m√•nga plattformar (CPU, GPU, mobil, edge). Det g√∂r modellen portabel och optimerad f√∂r inference.\n",
        "\n",
        "**F√∂rdelar**:\n",
        "- Snabbare inference √§n PyTorch\n",
        "- Mindre minnesanv√§ndning\n",
        "- Fungerar p√• Raspberry Pi\n",
        "- St√∂d f√∂r kvantisering (INT8)\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Ditt eget experiment\n",
        "\n",
        "**Uppgift**: Tr√§na en modell med andra inst√§llningar och j√§mf√∂r resultaten.\n",
        "\n",
        "**F√∂rslag**:\n",
        "- √ñka epochs till 3-5\n",
        "- √Ñndra batch_size till 64 eller 256\n",
        "- Testa med och utan `--no-pretrained`\n",
        "\n",
        "**Kod att modifiera**:\n",
        "```python\n",
        "# √Ñndra dessa v√§rden:\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 64\n",
        "USE_PRETRAINED = False  # True f√∂r f√∂rtr√§nade vikter\n",
        "\n",
        "!python -m piedge_edukit.train --fakedata --epochs {EPOCHS} --batch-size {BATCH_SIZE} --output-dir ./models_myexp\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementera ditt experiment h√§r\n",
        "# √Ñndra v√§rdena nedan och k√∂r tr√§ningen\n",
        "\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 64\n",
        "USE_PRETRAINED = False\n",
        "\n",
        "print(f\"üß™ Mitt experiment: epochs={EPOCHS}, batch_size={BATCH_SIZE}, pretrained={USE_PRETRAINED}\")\n",
        "\n",
        "# TODO: K√∂r tr√§ningen med dina inst√§llningar\n",
        "# !python -m piedge_edukit.train --fakedata --epochs {EPOCHS} --batch-size {BATCH_SIZE} --output-dir ./models_myexp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Sammanfattning\n",
        "\n",
        "Du har nu l√§rt dig:\n",
        "- Vad FakeData √§r och varf√∂r vi anv√§nder det\n",
        "- Hur tr√§ning fungerar med olika hyperparametrar\n",
        "- Varf√∂r ONNX-export √§r viktigt f√∂r edge deployment\n",
        "\n",
        "**N√§sta steg**: G√• till `02_latency_benchmark.ipynb` f√∂r att f√∂rst√• hur vi m√§ter modellens prestanda.\n",
        "\n",
        "**Viktiga begrepp**:\n",
        "- **Epochs**: Antal genomg√•ngar av datasetet\n",
        "- **Batch size**: Antal bilder per tr√§ningssteg\n",
        "- **Pretrained weights**: F√∂rtr√§nade vikter fr√•n ImageNet\n",
        "- **ONNX**: Standardformat f√∂r edge deployment\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
