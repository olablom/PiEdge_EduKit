{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Latency & Throughput Benchmarking\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "* Distinguish **latency** (time for one request) vs **throughput** (requests/second).\n",
        "* Understand **warm-up**: the first inferences are slower due to lazy initialization and caches.\n",
        "* Interpret percentiles (**P50**, **P95**, **P99**) and why tail latency matters.\n",
        "* See how **batch size** trades latency for throughput.\n",
        "\n",
        "## You Should Be Able To...\n",
        "\n",
        "- Explain why warm-up runs are necessary in latency benchmarking\n",
        "- Run benchmarks with different batch sizes and interpret results\n",
        "- Calculate and compare P50/P95 latency percentiles\n",
        "- Identify performance bottlenecks in model inference\n",
        "- Make informed decisions about batch size for deployment\n",
        "\n",
        "---\n",
        "\n",
        "## Concepts\n",
        "\n",
        "**Warm-up runs**: prime kernels, JITs, memory. Don't include them in metrics.\n",
        "\n",
        "**P50 vs P95**: P95 tells you about \"slow outliers\". SLAs often target a percentile, not the mean.\n",
        "\n",
        "**Providers/EPs**: same ONNX model, different backends (CPU/GPU/NNAPI).\n",
        "\n",
        "**Batch size**: larger batches can improve throughput but increase per-request latency.\n",
        "\n",
        "## Success Criteria\n",
        "\n",
        "* ‚úÖ Report shows mean/P50/P95 and a PNG plot\n",
        "* ‚úÖ You can explain whether latency distribution is tight or spiky\n",
        "* ‚úÖ You can justify a batch size for your target use case\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Environment Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ruff: noqa: E401\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Path.cwd().name == \"labs\":\n",
        "    os.chdir(Path.cwd().parent)\n",
        "    print(\"‚Üí Working dir set to repo root:\", os.getcwd())\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import onnxruntime as ort\n",
        "from piedge_edukit.preprocess import FakeData\n",
        "import piedge_edukit as _pkg  # noqa: F401\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment self-heal (Python 3.12 + editable install)\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "print(f\"Python: {sys.version.split()[0]} (need 3.12)\")\n",
        "\n",
        "try:\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"‚úÖ PiEdge EduKit package OK\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"‚ÑπÔ∏è Installing package in editable mode ‚Ä¶\")\n",
        "    root = os.getcwd()\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", root])\n",
        "    importlib.invalidate_caches()\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"‚úÖ Package installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All imports are now in the first cell above\n",
        "print(\"‚úÖ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept: Latency vs Throughput\n",
        "\n",
        "**Latency** measures how long a single inference takes (time per prediction).\n",
        "**Throughput** measures how many inferences can be processed per second.\n",
        "\n",
        "Key metrics:\n",
        "- **Mean latency**: Average time per inference\n",
        "- **P50 latency**: Median time (50% of inferences are faster)\n",
        "- **P95 latency**: 95th percentile (95% of inferences are faster)\n",
        "- **Warm-up**: Initial runs that \"prime\" the system (GPU memory allocation, JIT compilation, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task A: Explain Warm-up\n",
        "\n",
        "**Multiple Choice**: Why are warm-up runs important in latency benchmarking?\n",
        "\n",
        "A) They improve model accuracy\n",
        "B) They initialize system resources (GPU memory, JIT compilation, etc.)\n",
        "C) They reduce model size\n",
        "D) They increase throughput\n",
        "\n",
        "**Your answer**: _____\n",
        "\n",
        "**Short justification** (1-2 sentences): Why does this matter for accurate benchmarking?\n",
        "\n",
        "*Your answer here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task B: Batch Size Experiment\n",
        "\n",
        "Run benchmarks with different batch sizes and analyze the performance trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: run the benchmark with batch_size in {1, 8, 32}, runs=10\n",
        "# Record p50/p95 and explain the trend.\n",
        "\n",
        "def benchmark_batch_size(model_path, batch_sizes, runs=10, warmup=3):\n",
        "    \"\"\"\n",
        "    Benchmark model with different batch sizes.\n",
        "    Returns list of dicts with 'batch', 'p50', 'p95', 'mean' keys.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Load model once\n",
        "    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    output_name = session.get_outputs()[0].name\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"Benchmarking batch size {batch_size}...\")\n",
        "        \n",
        "        # Generate test data\n",
        "        fake_data = FakeData(num_samples=batch_size * runs, image_size=64, num_classes=2)\n",
        "        latencies = []\n",
        "        \n",
        "        # Warm-up runs\n",
        "        for _ in range(warmup):\n",
        "            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n",
        "            _ = session.run([output_name], {input_name: dummy_input})\n",
        "        \n",
        "        # Actual benchmark runs\n",
        "        for i in range(runs):\n",
        "            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            _ = session.run([output_name], {input_name: dummy_input})\n",
        "            end_time = time.time()\n",
        "            \n",
        "            latency_ms = (end_time - start_time) * 1000\n",
        "            latencies.append(latency_ms)\n",
        "        \n",
        "        # Calculate percentiles\n",
        "        p50 = np.percentile(latencies, 50)\n",
        "        p95 = np.percentile(latencies, 95)\n",
        "        mean_lat = np.mean(latencies)\n",
        "        \n",
        "        results.append({\n",
        "            'batch': batch_size,\n",
        "            'p50': p50,\n",
        "            'p95': p95,\n",
        "            'mean': mean_lat\n",
        "        })\n",
        "        \n",
        "        print(f\"  Batch {batch_size}: P50={p50:.2f}ms, P95={p95:.2f}ms, Mean={mean_lat:.2f}ms\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# TODO: Run the experiment\n",
        "# Hint: benchmark_batch_size(\"./models/student_model.onnx\", [1, 8, 32], runs=10)\n",
        "\n",
        "print(\"‚úÖ Benchmark function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the benchmark experiment\n",
        "model_path = \"./models/student_model.onnx\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"‚ùå Model not found. Please complete Notebook 01 first.\")\n",
        "    print(\"Expected path:\", model_path)\n",
        "else:\n",
        "    # TODO: Run the benchmark with batch sizes [1, 8, 32]\n",
        "    results = benchmark_batch_size(model_path, [1, 8, 32], runs=10)\n",
        "    \n",
        "    # Display results in a table\n",
        "    print(\"\\nüìä Benchmark Results:\")\n",
        "    print(\"Batch Size | P50 (ms) | P95 (ms) | Mean (ms)\")\n",
        "    print(\"-\" * 45)\n",
        "    for r in results:\n",
        "        print(f\"{r['batch']:10} | {r['p50']:8.2f} | {r['p95']:8.2f} | {r['mean']:8.2f}\")\n",
        "    \n",
        "    # Auto-check\n",
        "    assert len(results) >= 3 and all({'batch','p50','p95'} <= set(r) for r in results)\n",
        "    print(\"‚úÖ Results format OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "if 'results' in locals() and len(results) >= 3:\n",
        "    batch_sizes = [r['batch'] for r in results]\n",
        "    p50_values = [r['p50'] for r in results]\n",
        "    p95_values = [r['p95'] for r in results]\n",
        "    mean_values = [r['mean'] for r in results]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, p50_values, 'o-', label='P50 Latency', linewidth=2)\n",
        "    plt.plot(batch_sizes, p95_values, 's-', label='P95 Latency', linewidth=2)\n",
        "    plt.plot(batch_sizes, mean_values, '^-', label='Mean Latency', linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Batch Size')\n",
        "    plt.ylabel('Latency (ms)')\n",
        "    plt.title('Latency vs Batch Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"üìà Chart shows latency trends across batch sizes\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results to visualize. Run the benchmark first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Questions\n",
        "\n",
        "Based on your benchmark results, answer these questions:\n",
        "\n",
        "**1. How does latency change as batch size increases? Explain the trend.**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**2. Why might P95 latency be higher than P50 latency? What does this tell us about system performance?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**3. If you were deploying this model to a real-time application, which batch size would you choose and why?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Excellent work! You've learned how to measure and analyze model performance.\n",
        "\n",
        "**Next**: Open `03_quantization.ipynb` to learn about model compression and optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "- ‚úÖ Understood latency vs throughput concepts\n",
        "- ‚úÖ Implemented warm-up benchmarking\n",
        "- ‚úÖ Analyzed batch size effects on performance\n",
        "- ‚úÖ Interpreted P50/P95 latency metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ö° Latensbenchmark - F√∂rst√• modellens prestanda\n",
        "\n",
        "**M√•l**: F√∂rst√• hur vi m√§ter och tolkar modellens latens (svarstid).\n",
        "\n",
        "I detta notebook kommer vi att:\n",
        "- F√∂rst√• vad latens √§r och varf√∂r det √§r viktigt\n",
        "- Se hur benchmark fungerar (warmup, runs, providers)\n",
        "- Tolka resultat (p50, p95, histogram)\n",
        "- Experimentera med olika inst√§llningar\n",
        "\n",
        "> **üí° Tips**: Latens √§r avg√∂rande f√∂r edge deployment - en modell som √§r f√∂r l√•ngsam √§r inte anv√§ndbar i verkligheten!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Vad √§r latens och varf√∂r √§r det viktigt?\n",
        "\n",
        "**Latens** = tiden det tar f√∂r modellen att g√∂ra en f√∂ruts√§gelse (inference time).\n",
        "\n",
        "**Varf√∂r viktigt f√∂r edge**:\n",
        "- **Realtidsapplikationer** - robotar, autonoma fordon\n",
        "- **Anv√§ndarupplevelse** - ingen vill v√§nta 5 sekunder p√• en bildklassificering\n",
        "- **Resursbegr√§nsningar** - Raspberry Pi har begr√§nsad CPU/memory\n",
        "\n",
        "<details>\n",
        "<summary>üîç Klicka f√∂r att se typiska latensm√•l</summary>\n",
        "\n",
        "**Typiska latensm√•l**:\n",
        "- **< 10ms**: Realtidsvideo, gaming\n",
        "- **< 100ms**: Interaktiva applikationer\n",
        "- **< 1000ms**: Batch processing, offline analys\n",
        "\n",
        "**V√•r modell**: F√∂rv√§ntar oss ~1-10ms p√• CPU (bra f√∂r edge!)\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Hur fungerar benchmark?\n",
        "\n",
        "**Benchmark-processen**:\n",
        "1. **Warmup** - k√∂r modellen n√•gra g√•nger f√∂r att \"v√§rmma upp\" (JIT compilation, cache)\n",
        "2. **Runs** - m√§ter latens f√∂r m√•nga k√∂rningar\n",
        "3. **Statistik** - ber√§knar p50, p95, mean, std\n",
        "\n",
        "**Varf√∂r warmup?**\n",
        "- F√∂rsta k√∂rningen √§r ofta l√•ngsam (JIT compilation)\n",
        "- Cache-v√§rme p√•verkar prestanda\n",
        "- Vi vill m√§ta \"steady state\" prestanda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# K√∂r benchmark med olika inst√§llningar\n",
        "print(\"üöÄ K√∂r benchmark...\")\n",
        "\n",
        "# Anv√§nd modellen fr√•n f√∂reg√•ende notebook (eller skapa en snabb)\n",
        "!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_bench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark med olika antal runs f√∂r att se varians\n",
        "import os\n",
        "\n",
        "# Test 1: F√• runs (snabb)\n",
        "print(\"üìä Test 1: 10 runs\")\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup 3 --runs 10 --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visa benchmark-resultat\n",
        "if os.path.exists(\"./reports/latency_summary.txt\"):\n",
        "    with open(\"./reports/latency_summary.txt\", \"r\") as f:\n",
        "        print(\"üìà Benchmark-resultat:\")\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"‚ùå Benchmark-rapport saknas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L√§s detaljerade latensdata och visualisera\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if os.path.exists(\"./reports/latency.csv\"):\n",
        "    df = pd.read_csv(\"./reports/latency.csv\")\n",
        "    \n",
        "    print(f\"üìä Latens-statistik:\")\n",
        "    print(f\"Antal m√§tningar: {len(df)}\")\n",
        "    print(f\"Mean: {df['latency_ms'].mean():.2f} ms\")\n",
        "    print(f\"Std: {df['latency_ms'].std():.2f} ms\")\n",
        "    print(f\"Min: {df['latency_ms'].min():.2f} ms\")\n",
        "    print(f\"Max: {df['latency_ms'].max():.2f} ms\")\n",
        "    \n",
        "    # Histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df['latency_ms'], bins=20, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('Latens (ms)')\n",
        "    plt.ylabel('Antal')\n",
        "    plt.title('Latens-distribution')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    # Box plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(df['latency_ms'])\n",
        "    plt.ylabel('Latens (ms)')\n",
        "    plt.title('Latens Box Plot')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ùå Latens CSV saknas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Reflektionsfr√•gor\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ Varf√∂r √§r p95 viktigare √§n mean f√∂r edge deployment?</summary>\n",
        "\n",
        "**Svar**: p95 (95:e percentilen) visar den v√§rsta latensen som 95% av anv√§ndarna upplever. Det √§r viktigare √§n mean eftersom:\n",
        "\n",
        "- **Anv√§ndarupplevelse**: En anv√§ndare som f√•r 100ms latens kommer att m√§rka det, √§ven om mean √§r 10ms\n",
        "- **SLA-krav**: M√•nga system har SLA-krav p√• p95 latens\n",
        "- **Outliers**: Mean kan p√•verkas av extrema v√§rden, p95 √§r mer robust\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ Vad h√§nder med latens-variansen n√§r du √∂kar antal runs?</summary>\n",
        "\n",
        "**Svar**: Med fler runs f√•r vi:\n",
        "- **Mer stabil statistik** - p50/p95 blir mer tillf√∂rlitliga\n",
        "- **B√§ttre f√∂rst√•else av varians** - ser om modellen √§r konsistent\n",
        "- **Mindre p√•verkan av outliers** - enstaka l√•ngsamma k√∂rningar p√•verkar mindre\n",
        "\n",
        "**Experiment**: K√∂r benchmark med 10, 50, 100 runs och j√§mf√∂r standardavvikelsen.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Ditt eget experiment\n",
        "\n",
        "**Uppgift**: K√∂r benchmark med olika inst√§llningar och j√§mf√∂r resultaten.\n",
        "\n",
        "**F√∂rslag**:\n",
        "- Testa olika antal runs (10, 50, 100)\n",
        "- J√§mf√∂r warmup-effekten (0, 3, 10 warmup)\n",
        "- Analysera variansen mellan k√∂rningar\n",
        "\n",
        "**Kod att modifiera**:\n",
        "```python\n",
        "# √Ñndra dessa v√§rden:\n",
        "WARMUP_RUNS = 5\n",
        "BENCHMARK_RUNS = 50\n",
        "\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementera ditt experiment h√§r\n",
        "# √Ñndra v√§rdena nedan och k√∂r benchmark\n",
        "\n",
        "WARMUP_RUNS = 5\n",
        "BENCHMARK_RUNS = 50\n",
        "\n",
        "print(f\"üß™ Mitt experiment: warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}\")\n",
        "\n",
        "# TODO: K√∂r benchmark med dina inst√§llningar\n",
        "# !python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Sammanfattning\n",
        "\n",
        "Du har nu l√§rt dig:\n",
        "- Vad latens √§r och varf√∂r det √§r kritiskt f√∂r edge deployment\n",
        "- Hur benchmark fungerar (warmup, runs, statistik)\n",
        "- Hur man tolkar latens-resultat (p50, p95, varians)\n",
        "- Varf√∂r p95 √§r viktigare √§n mean f√∂r anv√§ndarupplevelse\n",
        "\n",
        "**N√§sta steg**: G√• till `03_quantization.ipynb` f√∂r att f√∂rst√• hur kvantisering kan f√∂rb√§ttra prestanda.\n",
        "\n",
        "**Viktiga begrepp**:\n",
        "- **Latens**: Inference-tid (kritiskt f√∂r edge)\n",
        "- **Warmup**: F√∂rbereder modellen f√∂r m√§tning\n",
        "- **p50/p95**: Percentiler f√∂r latens-distribution\n",
        "- **Varians**: Konsistens i prestanda\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
