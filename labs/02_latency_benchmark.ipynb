{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Latency & Throughput Benchmarking\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "* Distinguish **latency** (time for one request) vs **throughput** (requests/second).\n",
        "* Understand **warm-up**: the first inferences are slower due to lazy initialization and caches.\n",
        "* Interpret percentiles (**P50**, **P95**, **P99**) and why tail latency matters.\n",
        "* See how **batch size** trades latency for throughput.\n",
        "\n",
        "## You Should Be Able To...\n",
        "\n",
        "- Explain why warm-up runs are necessary in latency benchmarking\n",
        "- Run benchmarks with different batch sizes and interpret results\n",
        "- Calculate and compare P50/P95 latency percentiles\n",
        "- Identify performance bottlenecks in model inference\n",
        "- Make informed decisions about batch size for deployment\n",
        "\n",
        "---\n",
        "\n",
        "## Concepts\n",
        "\n",
        "**Warm-up runs**: prime kernels, JITs, memory. Don't include them in metrics.\n",
        "\n",
        "**P50 vs P95**: P95 tells you about \"slow outliers\". SLAs often target a percentile, not the mean.\n",
        "\n",
        "**Providers/EPs**: same ONNX model, different backends (CPU/GPU/NNAPI).\n",
        "\n",
        "**Batch size**: larger batches can improve throughput but increase per-request latency.\n",
        "\n",
        "## Success Criteria\n",
        "\n",
        "* ✅ Report shows mean/P50/P95 and a PNG plot\n",
        "* ✅ You can explain whether latency distribution is tight or spiky\n",
        "* ✅ You can justify a batch size for your target use case\n",
        "\n",
        "---\n",
        "\n",
        "## Setup & Environment Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ruff: noqa: E401\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if Path.cwd().name == \"labs\":\n",
        "    os.chdir(Path.cwd().parent)\n",
        "    print(\"→ Working dir set to repo root:\", os.getcwd())\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import onnxruntime as ort\n",
        "from piedge_edukit.preprocess import FakeData\n",
        "import piedge_edukit as _pkg  # noqa: F401\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment self-heal (Python 3.12 + editable install)\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "print(f\"Python: {sys.version.split()[0]} (need 3.12)\")\n",
        "\n",
        "try:\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"✅ PiEdge EduKit package OK\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"ℹ️ Installing package in editable mode …\")\n",
        "    root = os.getcwd()\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", root])\n",
        "    importlib.invalidate_caches()\n",
        "    import piedge_edukit  # noqa: F401\n",
        "    print(\"✅ Package installed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All imports are now in the first cell above\n",
        "print(\"✅ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept: Latency vs Throughput\n",
        "\n",
        "**Latency** measures how long a single inference takes (time per prediction).\n",
        "**Throughput** measures how many inferences can be processed per second.\n",
        "\n",
        "Key metrics:\n",
        "- **Mean latency**: Average time per inference\n",
        "- **P50 latency**: Median time (50% of inferences are faster)\n",
        "- **P95 latency**: 95th percentile (95% of inferences are faster)\n",
        "- **Warm-up**: Initial runs that \"prime\" the system (GPU memory allocation, JIT compilation, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task A: Explain Warm-up\n",
        "\n",
        "**Multiple Choice**: Why are warm-up runs important in latency benchmarking?\n",
        "\n",
        "A) They improve model accuracy\n",
        "B) They initialize system resources (GPU memory, JIT compilation, etc.)\n",
        "C) They reduce model size\n",
        "D) They increase throughput\n",
        "\n",
        "**Your answer**: _____\n",
        "\n",
        "**Short justification** (1-2 sentences): Why does this matter for accurate benchmarking?\n",
        "\n",
        "*Your answer here:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task B: Batch Size Experiment\n",
        "\n",
        "Run benchmarks with different batch sizes and analyze the performance trends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: run the benchmark with batch_size in {1, 8, 32}, runs=10\n",
        "# Record p50/p95 and explain the trend.\n",
        "\n",
        "def benchmark_batch_size(model_path, batch_sizes, runs=10, warmup=3):\n",
        "    \"\"\"\n",
        "    Benchmark model with different batch sizes.\n",
        "    Returns list of dicts with 'batch', 'p50', 'p95', 'mean' keys.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Load model once\n",
        "    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    output_name = session.get_outputs()[0].name\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"Benchmarking batch size {batch_size}...\")\n",
        "        \n",
        "        # Generate test data\n",
        "        fake_data = FakeData(num_samples=batch_size * runs, image_size=64, num_classes=2)\n",
        "        latencies = []\n",
        "        \n",
        "        # Warm-up runs\n",
        "        for _ in range(warmup):\n",
        "            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n",
        "            _ = session.run([output_name], {input_name: dummy_input})\n",
        "        \n",
        "        # Actual benchmark runs\n",
        "        for i in range(runs):\n",
        "            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            _ = session.run([output_name], {input_name: dummy_input})\n",
        "            end_time = time.time()\n",
        "            \n",
        "            latency_ms = (end_time - start_time) * 1000\n",
        "            latencies.append(latency_ms)\n",
        "        \n",
        "        # Calculate percentiles\n",
        "        p50 = np.percentile(latencies, 50)\n",
        "        p95 = np.percentile(latencies, 95)\n",
        "        mean_lat = np.mean(latencies)\n",
        "        \n",
        "        results.append({\n",
        "            'batch': batch_size,\n",
        "            'p50': p50,\n",
        "            'p95': p95,\n",
        "            'mean': mean_lat\n",
        "        })\n",
        "        \n",
        "        print(f\"  Batch {batch_size}: P50={p50:.2f}ms, P95={p95:.2f}ms, Mean={mean_lat:.2f}ms\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# TODO: Run the experiment\n",
        "# Hint: benchmark_batch_size(\"./models/student_model.onnx\", [1, 8, 32], runs=10)\n",
        "\n",
        "print(\"✅ Benchmark function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the benchmark experiment\n",
        "model_path = \"./models/student_model.onnx\"\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"❌ Model not found. Please complete Notebook 01 first.\")\n",
        "    print(\"Expected path:\", model_path)\n",
        "else:\n",
        "    # TODO: Run the benchmark with batch sizes [1, 8, 32]\n",
        "    results = benchmark_batch_size(model_path, [1, 8, 32], runs=10)\n",
        "    \n",
        "    # Display results in a table\n",
        "    print(\"\\n📊 Benchmark Results:\")\n",
        "    print(\"Batch Size | P50 (ms) | P95 (ms) | Mean (ms)\")\n",
        "    print(\"-\" * 45)\n",
        "    for r in results:\n",
        "        print(f\"{r['batch']:10} | {r['p50']:8.2f} | {r['p95']:8.2f} | {r['mean']:8.2f}\")\n",
        "    \n",
        "    # Auto-check\n",
        "    assert len(results) >= 3 and all({'batch','p50','p95'} <= set(r) for r in results)\n",
        "    print(\"✅ Results format OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "if 'results' in locals() and len(results) >= 3:\n",
        "    batch_sizes = [r['batch'] for r in results]\n",
        "    p50_values = [r['p50'] for r in results]\n",
        "    p95_values = [r['p95'] for r in results]\n",
        "    mean_values = [r['mean'] for r in results]\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(batch_sizes, p50_values, 'o-', label='P50 Latency', linewidth=2)\n",
        "    plt.plot(batch_sizes, p95_values, 's-', label='P95 Latency', linewidth=2)\n",
        "    plt.plot(batch_sizes, mean_values, '^-', label='Mean Latency', linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Batch Size')\n",
        "    plt.ylabel('Latency (ms)')\n",
        "    plt.title('Latency vs Batch Size')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"📈 Chart shows latency trends across batch sizes\")\n",
        "else:\n",
        "    print(\"⚠️ No results to visualize. Run the benchmark first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Questions\n",
        "\n",
        "Based on your benchmark results, answer these questions:\n",
        "\n",
        "**1. How does latency change as batch size increases? Explain the trend.**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**2. Why might P95 latency be higher than P50 latency? What does this tell us about system performance?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n",
        "\n",
        "---\n",
        "\n",
        "**3. If you were deploying this model to a real-time application, which batch size would you choose and why?**\n",
        "\n",
        "*Your answer here (2-3 sentences):*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Excellent work! You've learned how to measure and analyze model performance.\n",
        "\n",
        "**Next**: Open `03_quantization.ipynb` to learn about model compression and optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "- ✅ Understood latency vs throughput concepts\n",
        "- ✅ Implemented warm-up benchmarking\n",
        "- ✅ Analyzed batch size effects on performance\n",
        "- ✅ Interpreted P50/P95 latency metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ⚡ Latensbenchmark - Förstå modellens prestanda\n",
        "\n",
        "**Mål**: Förstå hur vi mäter och tolkar modellens latens (svarstid).\n",
        "\n",
        "I detta notebook kommer vi att:\n",
        "- Förstå vad latens är och varför det är viktigt\n",
        "- Se hur benchmark fungerar (warmup, runs, providers)\n",
        "- Tolka resultat (p50, p95, histogram)\n",
        "- Experimentera med olika inställningar\n",
        "\n",
        "> **💡 Tips**: Latens är avgörande för edge deployment - en modell som är för långsam är inte användbar i verkligheten!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Vad är latens och varför är det viktigt?\n",
        "\n",
        "**Latens** = tiden det tar för modellen att göra en förutsägelse (inference time).\n",
        "\n",
        "**Varför viktigt för edge**:\n",
        "- **Realtidsapplikationer** - robotar, autonoma fordon\n",
        "- **Användarupplevelse** - ingen vill vänta 5 sekunder på en bildklassificering\n",
        "- **Resursbegränsningar** - Raspberry Pi har begränsad CPU/memory\n",
        "\n",
        "<details>\n",
        "<summary>🔍 Klicka för att se typiska latensmål</summary>\n",
        "\n",
        "**Typiska latensmål**:\n",
        "- **< 10ms**: Realtidsvideo, gaming\n",
        "- **< 100ms**: Interaktiva applikationer\n",
        "- **< 1000ms**: Batch processing, offline analys\n",
        "\n",
        "**Vår modell**: Förväntar oss ~1-10ms på CPU (bra för edge!)\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Hur fungerar benchmark?\n",
        "\n",
        "**Benchmark-processen**:\n",
        "1. **Warmup** - kör modellen några gånger för att \"värmma upp\" (JIT compilation, cache)\n",
        "2. **Runs** - mäter latens för många körningar\n",
        "3. **Statistik** - beräknar p50, p95, mean, std\n",
        "\n",
        "**Varför warmup?**\n",
        "- Första körningen är ofta långsam (JIT compilation)\n",
        "- Cache-värme påverkar prestanda\n",
        "- Vi vill mäta \"steady state\" prestanda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kör benchmark med olika inställningar\n",
        "print(\"🚀 Kör benchmark...\")\n",
        "\n",
        "# Använd modellen från föregående notebook (eller skapa en snabb)\n",
        "!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_bench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark med olika antal runs för att se varians\n",
        "import os\n",
        "\n",
        "# Test 1: Få runs (snabb)\n",
        "print(\"📊 Test 1: 10 runs\")\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup 3 --runs 10 --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visa benchmark-resultat\n",
        "if os.path.exists(\"./reports/latency_summary.txt\"):\n",
        "    with open(\"./reports/latency_summary.txt\", \"r\") as f:\n",
        "        print(\"📈 Benchmark-resultat:\")\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"❌ Benchmark-rapport saknas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Läs detaljerade latensdata och visualisera\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if os.path.exists(\"./reports/latency.csv\"):\n",
        "    df = pd.read_csv(\"./reports/latency.csv\")\n",
        "    \n",
        "    print(f\"📊 Latens-statistik:\")\n",
        "    print(f\"Antal mätningar: {len(df)}\")\n",
        "    print(f\"Mean: {df['latency_ms'].mean():.2f} ms\")\n",
        "    print(f\"Std: {df['latency_ms'].std():.2f} ms\")\n",
        "    print(f\"Min: {df['latency_ms'].min():.2f} ms\")\n",
        "    print(f\"Max: {df['latency_ms'].max():.2f} ms\")\n",
        "    \n",
        "    # Histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df['latency_ms'], bins=20, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('Latens (ms)')\n",
        "    plt.ylabel('Antal')\n",
        "    plt.title('Latens-distribution')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    # Box plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(df['latency_ms'])\n",
        "    plt.ylabel('Latens (ms)')\n",
        "    plt.title('Latens Box Plot')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"❌ Latens CSV saknas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤔 Reflektionsfrågor\n",
        "\n",
        "<details>\n",
        "<summary>💭 Varför är p95 viktigare än mean för edge deployment?</summary>\n",
        "\n",
        "**Svar**: p95 (95:e percentilen) visar den värsta latensen som 95% av användarna upplever. Det är viktigare än mean eftersom:\n",
        "\n",
        "- **Användarupplevelse**: En användare som får 100ms latens kommer att märka det, även om mean är 10ms\n",
        "- **SLA-krav**: Många system har SLA-krav på p95 latens\n",
        "- **Outliers**: Mean kan påverkas av extrema värden, p95 är mer robust\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>💭 Vad händer med latens-variansen när du ökar antal runs?</summary>\n",
        "\n",
        "**Svar**: Med fler runs får vi:\n",
        "- **Mer stabil statistik** - p50/p95 blir mer tillförlitliga\n",
        "- **Bättre förståelse av varians** - ser om modellen är konsistent\n",
        "- **Mindre påverkan av outliers** - enstaka långsamma körningar påverkar mindre\n",
        "\n",
        "**Experiment**: Kör benchmark med 10, 50, 100 runs och jämför standardavvikelsen.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎯 Ditt eget experiment\n",
        "\n",
        "**Uppgift**: Kör benchmark med olika inställningar och jämför resultaten.\n",
        "\n",
        "**Förslag**:\n",
        "- Testa olika antal runs (10, 50, 100)\n",
        "- Jämför warmup-effekten (0, 3, 10 warmup)\n",
        "- Analysera variansen mellan körningar\n",
        "\n",
        "**Kod att modifiera**:\n",
        "```python\n",
        "# Ändra dessa värden:\n",
        "WARMUP_RUNS = 5\n",
        "BENCHMARK_RUNS = 50\n",
        "\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementera ditt experiment här\n",
        "# Ändra värdena nedan och kör benchmark\n",
        "\n",
        "WARMUP_RUNS = 5\n",
        "BENCHMARK_RUNS = 50\n",
        "\n",
        "print(f\"🧪 Mitt experiment: warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}\")\n",
        "\n",
        "# TODO: Kör benchmark med dina inställningar\n",
        "# !python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎉 Sammanfattning\n",
        "\n",
        "Du har nu lärt dig:\n",
        "- Vad latens är och varför det är kritiskt för edge deployment\n",
        "- Hur benchmark fungerar (warmup, runs, statistik)\n",
        "- Hur man tolkar latens-resultat (p50, p95, varians)\n",
        "- Varför p95 är viktigare än mean för användarupplevelse\n",
        "\n",
        "**Nästa steg**: Gå till `03_quantization.ipynb` för att förstå hur kvantisering kan förbättra prestanda.\n",
        "\n",
        "**Viktiga begrepp**:\n",
        "- **Latens**: Inference-tid (kritiskt för edge)\n",
        "- **Warmup**: Förbereder modellen för mätning\n",
        "- **p50/p95**: Percentiler för latens-distribution\n",
        "- **Varians**: Konsistens i prestanda\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
