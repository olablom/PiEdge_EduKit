{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ö° Kvantisering (INT8) - Komprimera modellen f√∂r snabbare inference\n",
        "\n",
        "**M√•l**: F√∂rst√• hur kvantisering fungerar och n√§r det √§r v√§rt det.\n",
        "\n",
        "I detta notebook kommer vi att:\n",
        "- F√∂rst√• vad kvantisering √§r (FP32 ‚Üí INT8)\n",
        "- Se hur det p√•verkar modellstorlek och latens\n",
        "- Experimentera med olika kalibreringsstorlekar\n",
        "- F√∂rst√• kompromisser (accuracy vs prestanda)\n",
        "\n",
        "> **üí° Tips**: Kvantisering √§r en av de viktigaste teknikerna f√∂r edge deployment - det kan g√∂ra modellen 4x snabbare!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Vad √§r kvantisering?\n",
        "\n",
        "**Kvantisering** = konvertera modellen fr√•n 32-bit flyttal (FP32) till 8-bit heltal (INT8).\n",
        "\n",
        "**F√∂rdelar**:\n",
        "- **4x mindre modellstorlek** (32 bit ‚Üí 8 bit)\n",
        "- **2-4x snabbare inference** (INT8 √§r snabbare att ber√§kna)\n",
        "- **Mindre minnesanv√§ndning** (viktigt f√∂r edge)\n",
        "\n",
        "**Kompromisser**:\n",
        "- **Accuracy-f√∂rlust** - modellen kan bli mindre exakt\n",
        "- **Kalibrering kr√§vs** - beh√∂ver representativ data f√∂r att hitta r√§tt skala\n",
        "\n",
        "<details>\n",
        "<summary>üîç Klicka f√∂r att se tekniska detaljer</summary>\n",
        "\n",
        "**Teknisk f√∂rklaring**:\n",
        "- FP32: 32 bit per vikt (4 bytes)\n",
        "- INT8: 8 bit per vikt (1 byte)\n",
        "- Kvantisering hittar r√§tt skala f√∂r varje vikt\n",
        "- Kalibrering anv√§nder representativ data f√∂r att optimera skalan\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# F√∂rst skapar vi en modell att kvantisera\n",
        "print(\"üöÄ Skapar modell f√∂r kvantisering...\")\n",
        "!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_quant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kontrollera ursprunglig modellstorlek\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./models_quant/model.onnx\"):\n",
        "    original_size = os.path.getsize(\"./models_quant/model.onnx\") / (1024*1024)\n",
        "    print(f\"üì¶ Ursprunglig modellstorlek: {original_size:.2f} MB\")\n",
        "else:\n",
        "    print(\"‚ùå Modell saknas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Experimentera med olika kalibreringsstorlekar\n",
        "\n",
        "**Kalibreringsstorlek** = antal bilder som anv√§nds f√∂r att hitta r√§tt skala f√∂r kvantisering.\n",
        "\n",
        "**St√∂rre kalibrering**:\n",
        "- ‚úÖ B√§ttre accuracy (mer representativ data)\n",
        "- ‚ùå L√§ngre kvantiserings-tid\n",
        "- ‚ùå Mer minne under kvantisering\n",
        "\n",
        "**Mindre kalibrering**:\n",
        "- ‚úÖ Snabbare kvantisering\n",
        "- ‚úÖ Mindre minnesanv√§ndning\n",
        "- ‚ùå Potentiellt s√§mre accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Liten kalibrering (snabb)\n",
        "print(\"‚ö° Test 1: Liten kalibrering (16 bilder)\")\n",
        "!python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visa kvantiseringsresultat\n",
        "if os.path.exists(\"./reports/quantization_summary.txt\"):\n",
        "    with open(\"./reports/quantization_summary.txt\", \"r\") as f:\n",
        "        print(\"üìä Kvantiseringsresultat:\")\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"‚ùå Kvantiseringsrapport saknas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# J√§mf√∂r modellstorlekar\n",
        "if os.path.exists(\"./models_quant/model.onnx\") and os.path.exists(\"./models_quant/model_static.onnx\"):\n",
        "    original_size = os.path.getsize(\"./models_quant/model.onnx\") / (1024*1024)\n",
        "    quantized_size = os.path.getsize(\"./models_quant/model_static.onnx\") / (1024*1024)\n",
        "    \n",
        "    print(f\"üì¶ Modellstorlekar:\")\n",
        "    print(f\"  Ursprunglig (FP32): {original_size:.2f} MB\")\n",
        "    print(f\"  Kvantiserad (INT8): {quantized_size:.2f} MB\")\n",
        "    print(f\"  Komprimering: {original_size/quantized_size:.1f}x\")\n",
        "else:\n",
        "    print(\"‚ùå Modellfiler saknas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark b√•da modellerna f√∂r att j√§mf√∂ra latens\n",
        "print(\"üöÄ Benchmark ursprunglig modell (FP32)...\")\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_quant/model.onnx --warmup 3 --runs 20 --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark kvantiserad modell (INT8)\n",
        "print(\"‚ö° Benchmark kvantiserad modell (INT8)...\")\n",
        "!python -m piedge_edukit.benchmark --fakedata --model-path ./models_quant/model_static.onnx --warmup 3 --runs 20 --providers CPUExecutionProvider\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# J√§mf√∂r latens-resultat\n",
        "import pandas as pd\n",
        "\n",
        "# L√§s b√•da benchmark-resultaten\n",
        "fp32_file = \"./reports/latency_summary.txt\"\n",
        "if os.path.exists(fp32_file):\n",
        "    with open(fp32_file, \"r\") as f:\n",
        "        fp32_content = f.read()\n",
        "    \n",
        "    # Extrahera mean latens fr√•n texten (enkel parsing)\n",
        "    lines = fp32_content.split('\\n')\n",
        "    fp32_mean = None\n",
        "    for line in lines:\n",
        "        if 'Mean' in line and 'ms' in line:\n",
        "            try:\n",
        "                fp32_mean = float(line.split(':')[1].strip().replace('ms', '').strip())\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "    \n",
        "    print(f\"üìä Latens-j√§mf√∂relse:\")\n",
        "    if fp32_mean:\n",
        "        print(f\"  FP32 (ursprunglig): {fp32_mean:.2f} ms\")\n",
        "    else:\n",
        "        print(f\"  FP32: Kunde inte parsa latens\")\n",
        "    \n",
        "    # TODO: L√§gg till INT8-latens h√§r n√§r den √§r tillg√§nglig\n",
        "    print(f\"  INT8 (kvantiserad): [kommer efter benchmark]\")\n",
        "else:\n",
        "    print(\"‚ùå Benchmark-rapport saknas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§î Reflektionsfr√•gor\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ N√§r √§r INT8-kvantisering v√§rt det?</summary>\n",
        "\n",
        "**Svar**: INT8 √§r v√§rt det n√§r:\n",
        "- **Latens √§r kritisk** - realtidsapplikationer, edge deployment\n",
        "- **Minne √§r begr√§nsat** - mobil, Raspberry Pi\n",
        "- **Accuracy-f√∂rlusten √§r acceptabel** - < 1-2% accuracy-f√∂rlust √§r ofta OK\n",
        "- **Batch size √§r liten** - kvantisering fungerar b√§st med sm√• batches\n",
        "\n",
        "**N√§r INTE v√§rt det**:\n",
        "- Accuracy √§r absolut kritisk\n",
        "- Du har gott om minne och CPU\n",
        "- Modellen √§r redan snabb nog\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>üí≠ Vilka risker finns med kvantisering?</summary>\n",
        "\n",
        "**Svar**: Huvudrisker:\n",
        "- **Accuracy-f√∂rlust** - modellen kan bli mindre exakt\n",
        "- **Kalibreringsdata** - beh√∂ver representativ data f√∂r bra kvantisering\n",
        "- **Edge cases** - extrema v√§rden kan orsaka problem\n",
        "- **Debugging** - kvantiserade modeller √§r sv√•rare att debugga\n",
        "\n",
        "**Minskning**:\n",
        "- Testa noggrant med riktig data\n",
        "- Anv√§nd olika kalibreringsstorlekar\n",
        "- Benchmark b√•de accuracy och latens\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Ditt eget experiment\n",
        "\n",
        "**Uppgift**: Testa olika kalibreringsstorlekar och j√§mf√∂r resultaten.\n",
        "\n",
        "**F√∂rslag**:\n",
        "- Testa kalibreringsstorlekar: 8, 16, 32, 64\n",
        "- J√§mf√∂r modellstorlek och latens\n",
        "- Analysera accuracy-f√∂rlust (om tillg√§nglig)\n",
        "\n",
        "**Kod att modifiera**:\n",
        "```python\n",
        "# √Ñndra dessa v√§rden:\n",
        "CALIB_SIZE = 32\n",
        "\n",
        "!python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size {CALIB_SIZE}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implementera ditt experiment h√§r\n",
        "# √Ñndra v√§rdena nedan och k√∂r kvantisering\n",
        "\n",
        "CALIB_SIZE = 32\n",
        "\n",
        "print(f\"üß™ Mitt experiment: kalibreringsstorlek={CALIB_SIZE}\")\n",
        "\n",
        "# TODO: K√∂r kvantisering med din inst√§llning\n",
        "# !python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size {CALIB_SIZE}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Sammanfattning\n",
        "\n",
        "Du har nu l√§rt dig:\n",
        "- Vad kvantisering √§r (FP32 ‚Üí INT8) och varf√∂r det √§r viktigt\n",
        "- Hur kalibreringsstorlek p√•verkar resultatet\n",
        "- Kompromisser mellan accuracy och prestanda\n",
        "- N√§r kvantisering √§r v√§rt det vs n√§r det inte √§r det\n",
        "\n",
        "**N√§sta steg**: G√• till `04_evaluate_and_verify.ipynb` f√∂r att f√∂rst√• automatiska checks och kvittogenerering.\n",
        "\n",
        "**Viktiga begrepp**:\n",
        "- **Kvantisering**: FP32 ‚Üí INT8 f√∂r snabbare inference\n",
        "- **Kalibrering**: Representativ data f√∂r att hitta r√§tt skala\n",
        "- **Komprimering**: 4x mindre modellstorlek\n",
        "- **Speedup**: 2-4x snabbare inference\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
