

# ==== notebooks\00_run_everything.ipynb ====



## cell 1 [code]

```python
# Bootstrap: Import helpers and create directories
import sys
from pathlib import Path

# Add repo root to Python path
repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from utils.nb_helpers import run_module, run_script
print("‚úÖ Notebook helpers loaded - ready to run pipeline!")
```


## cell 2 [markdown]

# PiEdge EduKit ‚Äî Guided Demo (Smoke Test)

## Why this notebook exists
A **guided demo** that runs the entire pipeline once so you can validate the environment and see the end-to-end flow before doing anything more advanced. It mirrors the mini-project "cutlery sorter" (Edge ML on Raspberry Pi): train a tiny classifier locally, export to ONNX, benchmark latency, attempt INT8 quantization, evaluate, and verify.

## Purpose
- Give a **practical overview** of a small Edge-ML workflow from code to measurable results.
- Show why **ONNX** matters (same model runs on PC and Pi).
- Teach how to read **latency metrics** (p50/p95) and why **warm-up** matters.
- Demonstrate that **INT8 quantization may fail** on some machines and that **FP32 fallback** is acceptable in this lesson.

## What you will learn
- The pipeline: **Train ‚Üí Export (ONNX) ‚Üí Benchmark ‚Üí Quantize ‚Üí Evaluate ‚Üí Verify**.
- How to interpret **p50/p95** and perform proper **warm-up** before timing.
- Differences between **FP32** and **INT8** (size/latency/compatibility).
- Where artifacts are saved and how they're used: `models/`, `reports/`, `progress/receipt.json`.

## What you will produce
- `models/model.onnx` ‚Äî exported model.
- `reports/training_curves.png` ‚Äî training curves (visible even with 1 epoch).
- `reports/latency_plot.png` ‚Äî latency measurement.
- `reports/quantization_comparison.png` ‚Äî FP32 vs INT8 comparison (FP32-only if INT8 fails).
- `reports/confusion_matrix.png` ‚Äî quick quality snapshot.
- `progress/receipt.json` ‚Äî **receipt** with PASS/FAIL and key metrics.

## Run modes
- **Smoke Test (default, fast):** 1 epoch, few measurements ‚Üí ~2‚Äì3 min. Good for sanity check.
- **Pretty Demo (optional):** 5 epochs, more measurements ‚Üí clearer curves & more stable stats (a few minutes extra). Provided via scripts and documented in `README.md` and `index.html`.

## Prerequisites
- **Python 3.12** inside the repo's local **`.venv`** (see README for activation).
- Run from the **repo root** (paths are relative).
- Everything runs on your PC. The Raspberry Pi comes later for the GPIO part.

## Time budget
- Smoke Test: ~2‚Äì3 minutes of active time.
- Pretty Demo: ~5‚Äì7 minutes.

## Success criteria
- Notebook completes without errors.
- Artifacts exist in `models/` and `reports/`.
- `progress/receipt.json` shows **PASS**.

> **Note:** On some Windows setups, ONNX Runtime **INT8** quantization can fail. That is **expected** here; the lesson automatically falls back to **FP32**.


## cell 4 [markdown]

# Before you run

**Why this notebook exists**
This is a *guided demo* that kicks off the full pipeline so you can verify the environment and see the end-to-end flow once.

**Learning goals (quick)**
- See the whole path once: **train ‚Üí export (ONNX) ‚Üí benchmark ‚Üí quantize ‚Üí evaluate ‚Üí verify**.
- Know what **ONNX** is (portable inference format) and why we export to it.
- Understand **latency metrics** (p50/p95) and why **warm-up** matters.
- Recognize that **INT8 may fail** on some machines and that **FP32 fallback is acceptable** in this lesson.

**Before you run**
- Use **Python 3.12** inside the repo‚Äôs **`.venv`** (see README quickstart).
- Keep the **repo root** as working directory; paths are relative.
- Expect **quiet output** with live timers; warnings are suppressed unless relevant.

**Success criteria**
- The notebook completes without errors and generates artifacts in `models/`, `reports/`, and a **PASS** receipt in `progress/receipt.json`.


## cell 5 [code]

```python
# pyright: reportMissingImports=false, reportUndefinedVariable=false, reportAttributeAccessIssue=false
import sys, os
sys.path.insert(0, os.getcwd())
sys.path.insert(0, os.path.join(os.getcwd(), "src"))
print("Python executable:", sys.executable)
print("Working directory:", os.getcwd())
print("sys.path configured for imports")
```


## cell 6 [markdown]

## üéõÔ∏è Run Mode Selection

Choose your execution mode:

**Smoke Test (default)**: Fast pipeline verification (1 epoch, 3 benchmark runs, 32 eval samples)
- ‚úÖ Quick completion (~2-3 minutes)
- ‚úÖ Shows 1-point training curves (with markers)
- ‚úÖ Perfect for environment verification
- ‚úÖ Gives PASS in verify.py

**Pretty Demo**: Nice graphs for classroom (5 epochs, 200 benchmark runs, 200 eval samples)
- üìà Clear training curves (5 points)
- üìä Stable confusion matrix
- ‚è±Ô∏è Takes ~5-7 minutes
- ‚úÖ Also gives PASS in verify.py

---

## TODO: Run the complete pipeline

This notebook demonstrates the full ML pipeline. Follow these steps:

1. **Train** a model using the training script
2. **Export** the model to ONNX format
3. **Benchmark** inference latency
4. **Quantize** to INT8 (may fail on some systems)
5. **Evaluate** model performance
6. **Verify** all artifacts are generated correctly

<details><summary>Hint</summary>
Each step generates artifacts in specific directories. Check `models/`, `reports/`, and `progress/` folders.
</details>

<details><summary>Solution</summary>
Run each cell in sequence. The pipeline will automatically generate all required artifacts and create a verification receipt.
</details>

---

# 00 - Run Everything (Demo)

## Learning Goals

* See the complete ML pipeline from training to verification
* Understand the purpose of each step in the workflow
* Verify that the environment is properly configured

## Concepts

**Pipeline flow**: train ‚Üí export ‚Üí benchmark ‚Üí quantize ‚Üí evaluate ‚Üí verify

**ONNX export**: converts PyTorch models to portable format for edge deployment

**Latency benchmarking**: measures inference performance with warm-up and percentiles

**Quantization**: reduces model precision (FP32 ‚Üí INT8) for faster inference

**Verification**: automated checks ensure all components work correctly

## Common Pitfalls

* Running without proper Python 3.12 environment setup
* Missing dependencies or incorrect package installation
* File path issues when not running from repo root
* Expecting perfect accuracy on synthetic data

## Success Criteria

* ‚úÖ All pipeline steps complete without errors
* ‚úÖ Artifacts generated in correct directories
* ‚úÖ Receipt shows PASS status
* ‚úÖ Can explain purpose of each pipeline step

## Reflection

After completing this demo, reflect on:
- Which step took the longest and why?
- What surprised you about the pipeline flow?
- How does this compare to other ML workflows you've seen?

---

# üöÄ PiEdge EduKit - Quick Run & Sanity Check

## What you'll learn today

* Train a tiny image classifier in PyTorch
* Export the model to **ONNX** (a portable format for deployment)
* Measure inference latency and interpret P50/P95
* (Try to) quantize to INT8 and understand why it may fail
* Evaluate the model and record a reproducible "receipt"

## Why this matters

Most real projects train in Python but deploy elsewhere (C++, mobile, web, embedded). ONNX lets us move models **out of Python** without rewriting the model by hand.

## How to use this notebook

This is a **smoke test**: it runs the whole pipeline end-to-end so your environment is correct. For learning and coding tasks, continue with **`01_training_and_export.ipynb`** ‚Üí **`04_evaluate_and_verify.ipynb`**.

---

## ONNX 101

**What is ONNX?**
ONNX (Open Neural Network Exchange) is an **open standard** for representing ML models as a graph of operators (Conv, Relu, MatMul‚Ä¶). Many frameworks can **export** to ONNX (PyTorch, TensorFlow) and many runtimes can **execute** ONNX (ONNX Runtime, TensorRT, CoreML Tools).

**Why ONNX?**

* **Portability**: train in Python, deploy in C++/C#/Java/JS, mobile or edge.
* **Performance**: runtimes fuse ops and call optimized backends (MKL, cuDNN).
* **Interoperability**: one model file can run across platforms with different "Execution Providers" (CPU, CUDA, DirectML, NNAPI‚Ä¶).

**Key terms**

* **Opset**: version of the operator set supported by runtimes. We export with a specific opset (e.g., 17).
* **Static vs dynamic shapes**: fixed sizes are simpler/faster; dynamic adds flexibility.
* **Execution Provider (EP)**: the backend used by ONNX Runtime (e.g., `CPUExecutionProvider`).
* **Pre/Post-processing**: steps around the model (resize, normalize, label mapping). These **aren't** part of the ONNX graph; the app must do the same steps.


## cell 7 [code]

```python
# Run Mode Configuration
import ipywidgets as widgets
from IPython.display import display

# Create radio buttons for run mode selection
mode_radio = widgets.RadioButtons(
    options=[
        ('Smoke Test (1 epoch, fast)', 'smoke'),
        ('Pretty Demo (5 epochs, nice graphs)', 'pretty')
    ],
    value='smoke',
    description='Run Mode:',
    style={'description_width': 'initial'}
)

# Display the radio buttons
display(mode_radio)

# Set parameters based on selection
if mode_radio.value == 'smoke':
    EPOCHS = 1
    BATCH_SIZE = 256
    WARMUP_RUNS = 1
    BENCHMARK_RUNS = 3
    EVAL_LIMIT = 32
    print("‚úÖ Smoke Test mode selected")
    print("   - Training: 1 epoch, batch-size 256")
    print("   - Benchmark: 1 warmup, 3 runs")
    print("   - Evaluation: 32 samples")
else:
    EPOCHS = 5
    BATCH_SIZE = 16
    WARMUP_RUNS = 50
    BENCHMARK_RUNS = 200
    EVAL_LIMIT = 200
    print("üìà Pretty Demo mode selected")
    print("   - Training: 5 epochs, batch-size 16")
    print("   - Benchmark: 50 warmup, 200 runs")
    print("   - Evaluation: 200 samples")

print(f"\nParameters set: epochs={EPOCHS}, batch_size={BATCH_SIZE}, warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}, limit={EVAL_LIMIT}")
```


## cell 9 [markdown]

## 1Ô∏è‚É£ Setup & Verification

First we check that the environment is correct:


## cell 10 [code]

```python
# Quiet noisy ORT quantizer log line (appears even with correct preprocessing)
import logging
for name in ("", "onnxruntime", "onnxruntime.quantization"):
    logging.getLogger(name).setLevel(logging.ERROR)
```


## cell 11 [code]

```python
# Make notebook run from repo root (not notebooks/ or labs/) + quiet mode
import os, sys, warnings
from pathlib import Path

def cd_repo_root():
    p = Path.cwd()
    for _ in range(5):  # climb up at most 5 levels
        if (p/"verify.py").exists() and (p/"scripts"/"evaluate_onnx.py").exists():
            if str(p) not in sys.path: sys.path.insert(0, str(p))
            if p != Path.cwd():
                os.chdir(p)
                print("-> Changed working dir to repo root:", os.getcwd())
            return
        p = p.parent
    raise RuntimeError("Could not locate repo root")

cd_repo_root()

# Quiet progress bars and some noisy warnings
os.environ.setdefault("TQDM_DISABLE", "1")  # hide tqdm progress bars
os.environ.setdefault("PYTHONWARNINGS", "ignore")
os.environ.setdefault("ORT_LOG_SEVERITY_LEVEL", "3")  # ORT info/warn -> quiet
warnings.filterwarnings("ignore", category=UserWarning, module="onnxruntime")
```


## cell 12 [code]

```python
# ruff: noqa: E401
# Cross-platform runner + live clock (no shell redirection needed)
import sys
import subprocess
import time
import threading
import shutil
from contextlib import contextmanager
from IPython.display import display

try:
    import ipywidgets as widgets
    _HAVE_WIDGETS = True
except Exception:
    _HAVE_WIDGETS = False

@contextmanager
def running_timer(label="Running‚Ä¶"):
    start = time.time()
    symbols = ["üïê","üïë","üïí","üïì","üïî","üïï","üïñ","üïó","üïò","üïô","üïö","üïõ"]
    stop = False

    if _HAVE_WIDGETS:
        w = widgets.HTML()
        display(w)
        def _tick():
            k = 0
            while not stop:
                w.value = f"<b>{symbols[k%12]}</b> {label} &nbsp; <code>{time.time()-start:.1f}s</code>"
                time.sleep(0.5); k += 1
        t = threading.Thread(target=_tick, daemon=True); t.start()
        try:
            yield
        finally:
            stop = True; t.join(timeout=0.2)
            w.value = f"‚úÖ Done ‚Äî <code>{time.time()-start:.1f}s</code>"
    else:
        width = shutil.get_terminal_size((80, 20)).columns
        def _tick():
            k = 0
            while not stop:
                msg = f"{symbols[k%12]} {label}  {time.time()-start:.1f}s"
                print("\r" + msg[:width].ljust(width), end="")
                time.sleep(0.5); k += 1
            print()
        t = threading.Thread(target=_tick, daemon=True); t.start()
        try:
            yield
        finally:
            stop = True; t.join(timeout=0.2)
            print(f"‚úÖ Done ‚Äî {time.time()-start:.1f}s")

def run_module(label, module, *args):
    """Run `python -m <module> <args>` cross-platform, capture output, raise on error."""
    with running_timer(label):
        cmd = [sys.executable, "-W", "ignore", "-m", module, *map(str, args)]
        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        print(proc.stdout)
        if proc.returncode != 0:
            raise RuntimeError(f"{module} exited with code {proc.returncode}")

def run_script(label, path, *args):
    """Run `python <path> <args>` cross-platform, capture output, raise on error."""
    with running_timer(label):
        cmd = [sys.executable, "-W", "ignore", path, *map(str, args)]
        proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        print(proc.stdout)
        if proc.returncode != 0:
            raise RuntimeError(f"{path} exited with code {proc.returncode}")
```


## cell 13 [code]

```python
# Environment check + self-healing (Python 3.12 + editable install)
import sys, os, importlib, subprocess
print(f"Python version: {sys.version}")
assert sys.version_info[:2] == (3, 12), f"Python 3.12 required, you have {sys.version_info[:2]}"

try:
    import piedge_edukit  # noqa: F401
    print("‚úÖ PiEdge EduKit package OK")
except ModuleNotFoundError:
    # Find repo root: if we are in labs/, go one level up
    repo_root = os.path.abspath(os.path.join(os.getcwd(), "..")) if os.path.basename(os.getcwd()) == "labs" else os.getcwd()
    print("‚ö† Package missing ‚Äì installing editable from:", repo_root)
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-e", repo_root])
    importlib.invalidate_caches()
    import piedge_edukit  # noqa: F401
    print("‚úÖ Package installed")
```


## cell 14 [code]

```python
# The package should already be installed by the cell above. Quick sanity check:
import piedge_edukit
print("‚úÖ Package imported ‚Äì continue!")
```


## cell 15 [markdown]

## 2Ô∏è‚É£ Training & ONNX Export

Training a small model with FakeData and exporting to ONNX:


## cell 16 [code]

```python
# Train model (quick run for demo)
run_module("Training (FakeData)",
           "piedge_edukit.train",
           "--fakedata", "--no-pretrained",
           "--epochs", 1, "--batch-size", 256,
           "--output-dir", "./models")
```


## cell 17 [code]

```python
# Check that the model was created
import os
if os.path.exists("./models/model.onnx"):
    size_mb = os.path.getsize("./models/model.onnx") / (1024*1024)
    print(f"‚úÖ ONNX model created: {size_mb:.1f} MB")
else:
    print("‚ùå ONNX model missing")

# Show training curves
from PIL import Image
from pathlib import Path
from IPython.display import display

training_plot = Path("reports/training_curves.png")
if training_plot.exists():
    print("\nüìà Training curves:")
    display(Image.open(training_plot))
else:
    print("\n‚ö†Ô∏è Training curves missing ‚Äì run training first.")
```


## cell 18 [markdown]

## 3Ô∏è‚É£ Latency Benchmark

Measuring how fast the model is on CPU:


## cell 19 [code]

```python
# Run benchmark (quick mode)
run_module("Benchmarking (CPU)",
           "piedge_edukit.benchmark",
           "--fakedata",
           "--model-path", "./models/model.onnx",
           "--warmup", 1, "--runs", 3,
           "--providers", "CPUExecutionProvider")
```


## cell 20 [code]

```python
# Show Benchmark results
if os.path.exists("./reports/latency_summary.txt"):
    with open("./reports/latency_summary.txt", "r") as f:
        print("üìä Benchmark results:")
        print(f.read())
else:
    print("‚ùå Benchmark report missing")
```


## cell 21 [markdown]

## 4Ô∏è‚É£ Quantization (INT8)

Compressing the model for faster inference:


## cell 22 [code]

```python
# Best Practice: Use real training images for calibration
# This ensures correct preprocessing and avoids ORT warnings
from pathlib import Path
import numpy as np
from PIL import Image

print("üìä Setting up calibration data (best practice: reuse real training images)")

# Create tiny calibration image set if data/train/ is missing
calib_dir = Path("data/train")
if not calib_dir.exists() or not any(calib_dir.rglob("*.png")):
    print("   Creating fallback calibration dataset...")
    for cls in ["class0", "class1"]:
        (calib_dir / cls).mkdir(parents=True, exist_ok=True)
        for i in range(16):  # 32 total (16 per class)
            # Synthetic but "real" PNG files
            arr = (np.random.rand(64, 64, 3) * 255).astype(np.uint8)
            Image.fromarray(arr).save(calib_dir / cls / f"sample_{i:02d}.png")
    print(f"‚úÖ Created 32 fallback calibration images in {calib_dir}")
    print(f"   Source: Synthetic PNG files (organized like real training data)")
else:
    num_samples = sum(1 for p in calib_dir.rglob("*.png"))
    print(f"‚úÖ Found {num_samples} existing images in {calib_dir}")
    print(f"   Source: Real training data")

print("   ‚Üí Using --data-path ensures correct preprocessing (no ORT warning!)")
```


## cell 23 [code]

```python
# Run quantization with REAL training images (correct preprocessing, no ORT warning!)
try:
    if calib_dir.exists():
        # Use real training data - same preprocessing as model was trained with
        run_module("Quantization (INT8 with real calibration data)",
                   "piedge_edukit.quantization",
                   "--data-path", str(calib_dir),
                   "--model-path", "./models/model.onnx",
                   "--calib-size", 32)
    else:
        # Fallback to FakeData (will show ORT warning)
        run_module("Quantization (INT8 attempt with FakeData)",
                   "piedge_edukit.quantization",
                   "--fakedata",
                   "--model-path", "./models/model.onnx",
                   "--calib-size", 16)
except RuntimeError as e:
    print("‚ö†Ô∏è Quantization step failed (OK for demo):", e)
```


## cell 24 [code]

```python
# Show quantization results
if os.path.exists("./reports/quantization_summary.txt"):
    with open("./reports/quantization_summary.txt", "r") as f:
        print("‚ö° Quantization results:")
        print(f.read())
else:
    print("‚ùå Quantization report missing")

# Clear note about INT8 failures
print("\n‚ÑπÔ∏è INT8 quantization may fail on some environments. In this lesson **FP32** is accepted; verify accepts fallback.")
```


## cell 25 [markdown]

## 5Ô∏è‚É£ Evaluation & Verification

Testing the model and generating receipt:


## cell 26 [code]

```python
# Run evaluation
from pathlib import Path

run_script("Evaluating ONNX",
           str(Path("scripts/evaluate_onnx.py").resolve()),
           "--model", "./models/model.onnx",
           "--fakedata", "--limit", 16)
```


## cell 27 [code]

```python
# Run verification and generate receipt
from pathlib import Path

run_script("Verifying & generating receipt", str(Path("verify.py").resolve()))
```


## cell 28 [code]

```python
# Show receipt
import json
if os.path.exists("./progress/receipt.json"):
    with open("./progress/receipt.json", "r") as f:
        receipt = json.load(f)
    print("üìã Verification receipt:")
    print(f"Status: {'‚úÖ PASS' if receipt['pass'] else '‚ùå FAIL'}")
    print(f"Timestamp: {receipt['timestamp']}")
    print("\nChecks:")
    for check in receipt['checks']:
        status = "‚úÖ" if check['ok'] else "‚ùå"
        print(f"  {status} {check['name']}: {check['reason']}")
else:
    print("‚ùå Receipt missing")

# Show confusion matrix
confusion_plot = Path("reports/confusion_matrix.png")
if confusion_plot.exists():
    print("\nüìä Confusion Matrix:")
    display(Image.open(confusion_plot))
else:
    print("\n‚ö†Ô∏è Confusion matrix missing ‚Äì run evaluation first.")
```


## cell 29 [markdown]

## üéâ Done!

You have now completed the entire PiEdge EduKit lesson! 

**Next step**: Go to `01_training_and_export.ipynb` to understand what happened during training.

**Generated files**:
- `models/model.onnx` - Trained model
- `reports/` - Benchmark and quantization reports
- `progress/receipt.json` - Verification receipt

---

## üìö Continue with detailed lessons

**‚≠ê Recommended order**:
1. **`01_training_and_export.ipynb`** - Understand training and ONNX export
2. **`02_latency_benchmark.ipynb`** - Learn to measure performance
3. **`03_quantization.ipynb`** - Compress models for edge
4. **`04_evaluate_and_verify.ipynb`** - Evaluate and verify results

**üí° Tip**: Each notebook builds on the previous - run them in order for best learning!


## cell 31 [markdown]

## Mini-Glossary

* **ONNX**: portable model format defined as an operator graph.
* **Opset**: versioned set of operators supported by runtimes.
* **Execution Provider (EP)**: backend used by ONNX Runtime (CPU, CUDA, DirectML‚Ä¶).
* **Latency**: time per request; **Throughput**: requests per second.
* **P50/P95/P99**: latency percentiles; tails indicate rare slow requests.
* **Quantization (PTQ)**: convert FP32 to INT8 using calibration data.
* **Calibration**: running representative samples to estimate activation ranges.


# ==== notebooks\01_training_and_export.ipynb ====



## cell 1 [code]

```python
# Bootstrap: Import helpers and create directories
import sys
from pathlib import Path

# Add repo root to Python path
repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from utils.nb_helpers import run_module, run_script
print("‚úÖ Notebook helpers loaded - ready for training!")
```


## cell 2 [markdown]

# 01 - Training & Export

## Learning Goals

* Understand the pieces of a PyTorch training loop (model ‚Üí loss ‚Üí optimizer ‚Üí data loader ‚Üí epochs).
* Implement/inspect a tiny CNN and see how accuracy changes with hyper-parameters.
* Export to ONNX and verify the file loads and produces outputs of the right shape/dtype.

## You Should Be Able To...

- Implement a basic CNN using PyTorch layers
- Write a training loop with loss calculation and accuracy tracking
- Export a trained model to ONNX format with proper input/output specifications
- Explain why ONNX export is useful for edge deployment
- Identify key hyperparameters that affect model performance

---

## Concepts

**Training loop**: forward ‚Üí compute loss ‚Üí backward ‚Üí optimizer step ‚Üí repeat.

**Evaluation mode**: `model.eval()` disables dropout/batchnorm updates for deterministic inference.

**Export to ONNX**: we trace the model with a sample input and save `models/model.onnx`.

**Preprocessing contract**: whatever normalization/resizing you used during training **must be used at inference** (outside the ONNX graph).

## Common Pitfalls

* Forgetting `model.eval()` before export (exporting training behavior).
* Mismatch between training normalization and inference normalization (bad accuracy).
* Exporting with the wrong input shape.

## Success Criteria

* ‚úÖ Training runs and prints accuracy
* ‚úÖ `models/model.onnx` exists and can be loaded
* ‚úÖ Checker says shapes/dtypes are valid

---

## Setup & Environment Check


## cell 3 [code]

```python
# ruff: noqa: E401
import os
import sys
from pathlib import Path

# Ensure repo root in path if opened from labs/
if Path.cwd().name == "labs":
    os.chdir(Path.cwd().parent)
    print("‚Üí Working dir set to repo root:", os.getcwd())
if os.getcwd() not in sys.path:
    sys.path.insert(0, os.getcwd())

# Core deps
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import onnx
from onnx import checker  # noqa: F401
import onnxruntime as ort  # noqa: F401
import matplotlib.pyplot as plt
from torchvision.datasets import FakeData as TVFakeData

# Project package
from piedge_edukit.preprocess import FakeData as PEDFakeData

# Hints & Solutions helper (pure Jupyter, no extra deps)
from IPython.display import Markdown, display

def hints(*lines, solution: str | None = None, title="Need a nudge?"):
    """Render progressive hints + optional collapsible solution."""
    md = [f"### {title}"]
    for i, txt in enumerate(lines, start=1):
        md.append(f"<details><summary>Hint {i}</summary>\n\n{txt}\n\n</details>")
    if solution:
        # keep code fenced as python for readability
        md.append(
            "<details><summary><b>Show solution</b></summary>\n\n"
            f"```python\n{solution.strip()}\n```\n"
            "</details>"
        )
    display(Markdown("\n\n".join(md)))
```


## cell 4 [code]

```python
# Environment self-heal (Python 3.12 + editable install)
import subprocess
import importlib

print(f"Python: {sys.version.split()[0]} (need 3.12)")

try:
    import piedge_edukit  # noqa: F401
    print("‚úÖ PiEdge EduKit package OK")
except ModuleNotFoundError:
    print("‚ÑπÔ∏è Installing package in editable mode ‚Ä¶")
    root = os.getcwd()
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-e", root])
    importlib.invalidate_caches()
    import piedge_edukit  # noqa: F401
    print("‚úÖ Package installed")
```


## cell 5 [code]

```python
# All imports are now in the first cell above
print("‚úÖ All imports successful")
```


## cell 6 [markdown]

## Concept: Convolutional Neural Networks

CNNs are designed to process grid-like data (images) by:
- **Convolutional layers**: Learn spatial patterns (edges, textures, shapes)
- **Pooling layers**: Reduce spatial dimensions while preserving important features
- **Fully connected layers**: Make final classification decisions

For 64√ó64 RGB images, a typical architecture flows: `[3,64,64] ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Conv ‚Üí ReLU ‚Üí Pool ‚Üí Flatten ‚Üí Linear ‚Üí Linear ‚Üí [num_classes]`


## cell 7 [markdown]

## Task A: Implement a Simple CNN

Your task is to implement a `TinyCNN` class that can classify 64√ó64 RGB images into 2 classes.

### TODO A1 ‚Äî Implement `TinyCNN`
**Goal:** Build a minimal Conv ‚Üí ReLU ‚Üí MaxPool stack ending in a linear head.

<details><summary>Hint 1</summary>
Start with 3√ó3 conv, stride=1, padding=1. Use MaxPool 2√ó2 to downsample.
</details>

<details><summary>Hint 2</summary>
Two conv blocks are enough for FakeData. Flatten before the linear layer.
</details>

<details><summary>Hint 3</summary>
`forward(x)` should return logits (no softmax).
</details>

<details><summary>Solution</summary>

```python
import torch.nn as nn

class TinyCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.head = nn.Linear(32*8*8, num_classes)

    def forward(self, x):
        x = self.net(x)
        x = x.view(x.size(0), -1)
        return self.head(x)
```

</details>


## cell 8 [code]

```python
# TODO A1: implement TinyCNN here (or edit if already stubbed)

# Create model instance
model = TinyCNN(num_classes=2)
print(f"Model created with {sum(p.numel() for p in model.parameters())} parameters")
```


## cell 9 [code]

```python
# TEST: model should accept [1,3,64,64] and output [1,2]
# (torch already imported in first cell)
x = torch.randn(1,3,64,64)
y = model(x)
assert y.shape == (1,2), f"Expected (1,2), got {tuple(y.shape)}"
print("‚úÖ Shape test passed")
print(f"Input shape: {x.shape}")
print(f"Output shape: {y.shape}")
print(f"Output range: [{y.min().item():.3f}, {y.max().item():.3f}]")
```


## cell 10 [markdown]

## Concept: Training Loop Components

A typical training loop includes:
1. **Forward pass**: Compute predictions
2. **Loss calculation**: Compare predictions to ground truth
3. **Backward pass**: Compute gradients
4. **Optimizer step**: Update model parameters
5. **Metrics tracking**: Monitor loss and accuracy


## cell 11 [markdown]

## Task B: Write the Training Step

Implement a `train_one_epoch` function that trains the model for one epoch and returns loss and accuracy metrics.

### TODO B1 ‚Äî Implement one training step
**Goal:** zero_grad ‚Üí forward ‚Üí compute loss ‚Üí backward ‚Üí step

<details><summary>Hint 1</summary>
`optimizer.zero_grad()` must be called before `loss.backward()`.
</details>

<details><summary>Hint 2</summary>
Use `model.train()` during training.
</details>

<details><summary>Solution</summary>

```python
def train_step(model, batch, optimizer, criterion):
    model.train()
    x, y = batch
    optimizer.zero_grad()
    logits = model(x)
    loss = criterion(logits, y)
    loss.backward()
    optimizer.step()
    return float(loss.detach().cpu())
```

</details>


## cell 12 [code]

```python
# TODO B1: implement train_step(...)

# Test the function signature
print("‚úÖ Function signature looks correct")
```


## cell 13 [code]

```python
# Create test data and test the training function
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Create fake data loader
fake_data = FakeData(num_samples=100, image_size=64, num_classes=2)
train_loader = DataLoader(fake_data, batch_size=16, shuffle=True)

# Create optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Test training function
metrics = train_one_epoch(model, train_loader, optimizer, device)
assert "loss" in metrics and "acc" in metrics
print("‚úÖ Training loop smoke test passed")
print(f"Loss: {metrics['loss']:.4f}, Accuracy: {metrics['acc']:.2f}%")
```


## cell 14 [markdown]

## Concept: ONNX Export

ONNX (Open Neural Network Exchange) is a format that allows models to run on different platforms:
- **Cross-platform**: Same model runs on CPU, GPU, mobile, edge devices
- **Optimized inference**: ONNX Runtime provides optimized execution
- **Language agnostic**: Models can be used from Python, C++, C#, JavaScript, etc.

Key requirements for export:
- Model must be in evaluation mode (`model.eval()`)
- Provide a dummy input with correct shape
- Specify input/output names for clarity


## cell 15 [markdown]

## Task C: Export to ONNX

Export your trained model to ONNX format for edge deployment.

### TODO C1 ‚Äî Export to ONNX with dynamic axes
**Goal:** Put model in `eval()`, feed a dummy input, export to `models/model.onnx`.

<details><summary>Hint 1</summary>
Use `torch.onnx.export(model, dummy, "models/model.onnx", opset_version=17, dynamic_axes=...)`.
</details>

<details><summary>Hint 2</summary>
`dynamic_axes={"input": {0: "batch"}, "output": {0: "batch"}}`.
</details>

<details><summary>Solution</summary>

```python
import torch, os
os.makedirs("models", exist_ok=True)
model.eval()
dummy = torch.randn(1, 3, 32, 32)
torch.onnx.export(
    model, dummy, "models/model.onnx",
    input_names=["input"], output_names=["output"],
    dynamic_axes={"input": {0: "batch"}, "output": {0: "batch"}},
    opset_version=17
)
print("[OK] Exported to models/model.onnx")
```

</details>


## cell 16 [code]

```python
# TODO C1: export ONNX here

print("‚úÖ ONNX export completed")
```


## cell 17 [code]

```python
# Test ONNX export
# (onnx and os already imported in first cell)
assert os.path.exists("./models/model.onnx"), "ONNX file missing"
m = onnx.load("./models/model.onnx")
onnx.checker.check_model(m)
print("‚úÖ ONNX export verified")

# Show model info
file_size = os.path.getsize("./models/model.onnx") / (1024*1024)
print(f"Model size: {file_size:.2f} MB")
print(f"Input shape: {[d.dim_value for d in m.graph.input[0].type.tensor_type.shape.dim]}")
print(f"Output shape: {[d.dim_value for d in m.graph.output[0].type.tensor_type.shape.dim]}")
```


## cell 18 [markdown]

## Reflection Questions

Please answer these questions in 2-3 sentences each:


## cell 19 [markdown]

**1. What two hyperparameters most affected your validation accuracy? Why?**

*Your answer here (2-3 sentences):*

---

**2. Why is exporting to ONNX useful for edge deployment?**

*Your answer here (2-3 sentences):*

---

**3. What would happen if you forgot to call `model.eval()` before ONNX export?**

*Your answer here (2-3 sentences):*


## cell 20 [markdown]

## Next Steps

Great work! You've implemented a CNN, trained it, and exported it to ONNX format.

**Next**: Open `02_latency_benchmark.ipynb` to learn about performance measurement and optimization.

---

### Summary
- ‚úÖ Implemented TinyCNN architecture
- ‚úÖ Created training loop with metrics
- ‚úÖ Exported model to ONNX format
- ‚úÖ Verified export integrity


## cell 21 [markdown]

# üß† Training & ONNX Export - Understand what's happening

**Goal**: Understand how training works and experiment with different settings.

In this notebook we will:
- Understand what FakeData is and why we use it
- See how dataset-pipeline ‚Üí model ‚Üí loss/accuracy works
- Experiment with different hyperparameters
- Understand why we export to ONNX

> **üí° Tip**: Run the cells in order and read the explanations. Feel free to experiment with the values!


## cell 22 [markdown]

## ü§î What is FakeData and why do we use it?

**FakeData** are synthetic images that PyTorch generates automatically. It's perfect for:
- **Quick prototyping** - no downloading of large datasets
- **Reproducibility** - same data every time
- **Teaching** - focus on algorithms, not data management

<details>
<summary>üîç Click to see what FakeData contains</summary>

```python
# FakeData generates:
# - Random RGB images (64x64 pixels)
# - Random classes (0, 1, 2, ...)
# - Same structure as real image datasets
```

</details>


## cell 23 [code]

```python
# Let's create a small FakeData to see what it contains
import torch
from torchvision import datasets
import matplotlib.pyplot as plt

# Create FakeData with 2 classes
fake_data = datasets.FakeData(size=10, num_classes=2, transform=None)

# Show first image
image, label = fake_data[0]
print(f"Image size: {image.size}")
print(f"Class: {label}")
print(f"Pixel values: {image.getextrema()}")

# Show the image
plt.figure(figsize=(6, 4))
plt.imshow(image)
plt.title(f"FakeData - Class {label}")
plt.axis('off')
plt.show()
```


## cell 24 [markdown]

## üéØ Experiment with Training

Now we'll train a model and see how different settings affect the results.

**Hyperparameters to experiment with**:
- `epochs` - number of passes through the dataset
- `batch_size` - number of images per training step
- `--no-pretrained` - start from scratch vs pretrained weights


## cell 25 [code]

```python
# Experiment 1: Quick training (1 epoch, no pretrained)
print("üß™ Experiment 1: Quick training")
!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 128 --output-dir ./models_exp1
```


## cell 26 [code]

```python
# Show training results from Experiment 1
import json
import os

if os.path.exists("./models_exp1/training_info.json"):
    with open("./models_exp1/training_info.json", "r") as f:
        info = json.load(f)
    
    print("üìä Training results (Experiment 1):")
    print(f"Final accuracy: {info.get('final_accuracy', 'N/A'):.3f}")
    print(f"Final loss: {info.get('final_loss', 'N/A'):.3f}")
    print(f"Epochs: {info.get('epochs', 'N/A')}")
    print(f"Batch size: {info.get('batch_size', 'N/A')}")
else:
    print("‚ùå Training info missing")
```


## cell 27 [markdown]

## ü§î Reflection Questions

<details>
<summary>üí≠ What happens with overfitting when you increase epochs?</summary>

**Answer**: With more epochs, the model can learn the training data too well and generalize poorly to new data. This is called overfitting.

**Experiment**: Run the same training but with `--epochs 5` and compare accuracy on training vs validation data.

</details>

<details>
<summary>üí≠ Why do we export to ONNX (for Pi/edge)?</summary>

**Answer**: ONNX is a standard format that works on many platforms (CPU, GPU, mobile, edge). It makes the model portable and optimized for inference.

**Benefits**:
- Faster inference than PyTorch
- Less memory usage
- Works on Raspberry Pi
- Support for quantization (INT8)

</details>


## cell 28 [markdown]

## üéØ Your own experiment

**Task**: Train a model with different settings and compare the results.

**Suggestions**:
- Increase epochs to 3-5
- Change batch_size to 64 or 256
- Test with and without `--no-pretrained`

**Code to modify**:
```python
# Change these values:
EPOCHS = 3
BATCH_SIZE = 64
USE_PRETRAINED = False  # True for pretrained weights

!python -m piedge_edukit.train --fakedata --epochs {EPOCHS} --batch-size {BATCH_SIZE} --output-dir ./models_myexp
```


## cell 29 [code]

```python
# TODO: Implement your experiment here
# Change the values below and run the training

EPOCHS = 3
BATCH_SIZE = 64
USE_PRETRAINED = False

print(f"üß™ My experiment: epochs={EPOCHS}, batch_size={BATCH_SIZE}, pretrained={USE_PRETRAINED}")

# TODO: Run the training with your settings
# !python -m piedge_edukit.train --fakedata --epochs {EPOCHS} --batch-size {BATCH_SIZE} --output-dir ./models_myexp
```


## cell 30 [markdown]

## üéâ Summary

You have now learned:
- What FakeData is and why we use it
- How training works with different hyperparameters
- Why ONNX export is important for edge deployment

**Next step**: Go to `02_latency_benchmark.ipynb` to understand how we measure model performance.

**Key concepts**:
- **Epochs**: Number of passes through the dataset
- **Batch size**: Number of images per training step
- **Pretrained weights**: Pre-trained weights from ImageNet
- **ONNX**: Standard format for edge deployment


# ==== notebooks\02_latency_benchmark.ipynb ====



## cell 1 [code]

```python
# Bootstrap: Import helpers and create directories
import sys
from pathlib import Path

# Add repo root to Python path
repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from utils.nb_helpers import run_module, run_script
print("‚úÖ Notebook helpers loaded - ready for benchmarking!")
```


## cell 2 [markdown]

# 02 - Latency & Throughput Benchmarking

## Learning Goals

* Distinguish **latency** (time for one request) vs **throughput** (requests/second).
* Understand **warm-up**: the first inferences are slower due to lazy initialization and caches.
* Interpret percentiles (**P50**, **P95**, **P99**) and why tail latency matters.
* See how **batch size** trades latency for throughput.

## You Should Be Able To...

- Explain why warm-up runs are necessary in latency benchmarking
- Run benchmarks with different batch sizes and interpret results
- Calculate and compare P50/P95 latency percentiles
- Identify performance bottlenecks in model inference
- Make informed decisions about batch size for deployment

---

## Concepts

**Warm-up runs**: prime kernels, JITs, memory. Don't include them in metrics.

**P50 vs P95**: P95 tells you about "slow outliers". SLAs often target a percentile, not the mean.

**Providers/EPs**: same ONNX model, different backends (CPU/GPU/NNAPI).

**Batch size**: larger batches can improve throughput but increase per-request latency.

## Common Pitfalls

* Measuring latency without warm-up runs (first runs are slower)
* Using mean latency instead of percentiles for SLA planning
* Not considering batch size impact on single-request latency
* Ignoring system variability in benchmark results

## Success Criteria

* ‚úÖ Report shows mean/P50/P95 and a PNG plot
* ‚úÖ You can explain whether latency distribution is tight or spiky
* ‚úÖ You can justify a batch size for your target use case

## Reflection

After completing this notebook, reflect on:
- How did batch size affect latency vs throughput?
- Why is P95 latency more important than mean for user experience?
- What factors contribute to latency variability?

---

## Setup & Environment Check


## cell 3 [code]

```python
# ruff: noqa: E401
import os
import sys
from pathlib import Path

if Path.cwd().name == "labs":
    os.chdir(Path.cwd().parent)
    print("‚Üí Working dir set to repo root:", os.getcwd())
if os.getcwd() not in sys.path:
    sys.path.insert(0, os.getcwd())

import time
import numpy as np
import matplotlib.pyplot as plt
import onnxruntime as ort
from piedge_edukit.preprocess import FakeData as PEDFakeData
import piedge_edukit as _pkg  # noqa: F401

# Hints & Solutions helper (pure Jupyter, no extra deps)
from IPython.display import Markdown, display

def hints(*lines, solution: str | None = None, title="Need a nudge?"):
    """Render progressive hints + optional collapsible solution."""
    md = [f"### {title}"]
    for i, txt in enumerate(lines, start=1):
        md.append(f"<details><summary>Hint {i}</summary>\n\n{txt}\n\n</details>")
    if solution:
        # keep code fenced as python for readability
        md.append(
            "<details><summary><b>Show solution</b></summary>\n\n"
            f"```python\n{solution.strip()}\n```\n"
            "</details>"
        )
    display(Markdown("\n\n".join(md)))
```


## cell 4 [code]

```python
# Environment self-heal (Python 3.12 + editable install)
import subprocess
import importlib

print(f"Python: {sys.version.split()[0]} (need 3.12)")

try:
    import piedge_edukit  # noqa: F401
    print("‚úÖ PiEdge EduKit package OK")
except ModuleNotFoundError:
    print("‚ÑπÔ∏è Installing package in editable mode ‚Ä¶")
    root = os.getcwd()
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-e", root])
    importlib.invalidate_caches()
    import piedge_edukit  # noqa: F401
    print("‚úÖ Package installed")
```


## cell 5 [code]

```python
# All imports are now in the first cell above
print("‚úÖ All imports successful")
```


## cell 6 [markdown]

## Concept: Latency vs Throughput

**Latency** measures how long a single inference takes (time per prediction).
**Throughput** measures how many inferences can be processed per second.

Key metrics:
- **Mean latency**: Average time per inference
- **P50 latency**: Median time (50% of inferences are faster)
- **P95 latency**: 95th percentile (95% of inferences are faster)
- **Warm-up**: Initial runs that "prime" the system (GPU memory allocation, JIT compilation, etc.)


## cell 7 [markdown]

### TODO A1 ‚Äî Why warm-up?
Write 2‚Äì3 sentences explaining graph initialization, JIT/caches and memory allocation effects on first iterations.

<details><summary>Solution</summary>
Warm-up amortizes one-time costs (kernel/JIT init, memory allocation, cache fills) so measured latency reflects steady-state. Without warm-up, p50/p95 overestimates real throughput.
</details>

hints(
    "The first inferences include one-off costs (graph init, memory alloc).",
    "Warm-up runs stabilize timing so measured latencies reflect steady-state.",
    solution="""\
Warm-up eliminates one-time overhead (graph compilation/init, allocator warm-up).
It makes the reported latencies representative of steady-state performance."""
)

## Task A: Explain Warm-up

**Multiple Choice**: Why are warm-up runs important in latency benchmarking?

A) They improve model accuracy
B) They initialize system resources (GPU memory, JIT compilation, etc.)
C) They reduce model size
D) They increase throughput

**Your answer**: _____

**Short justification** (1-2 sentences): Why does this matter for accurate benchmarking?

*Your answer here:*


## cell 8 [markdown]

## Task B: Batch Size Experiment

Run benchmarks with different batch sizes and analyze the performance trends.


## cell 9 [code]

```python
# TODO B1: run latency for batch_sizes = [1, 2, 4, 8], collect p50/p95
# Plot/print a small table and briefly interpret which batch best fits *latency-first* deployments.

def benchmark_batch_size(model_path, batch_sizes, runs=10, warmup=3):
    """
    Benchmark model with different batch sizes.
    Returns list of dicts with 'batch', 'p50', 'p95', 'mean' keys.
    """
    results = []
    
    # Load model once
    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    
    for batch_size in batch_sizes:
        print(f"Benchmarking batch size {batch_size}...")
        
        # Generate test data
        fake_data = PEDFakeData(num_samples=batch_size * runs, image_size=64, num_classes=2)
        latencies = []
        
        # Warm-up runs
        for _ in range(warmup):
            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)
            _ = session.run([output_name], {input_name: dummy_input})
        
        # Actual benchmark runs
        for i in range(runs):
            dummy_input = np.random.randn(batch_size, 3, 64, 64).astype(np.float32)
            
            start_time = time.time()
            _ = session.run([output_name], {input_name: dummy_input})
            end_time = time.time()
            
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
        
        # Calculate percentiles
        p50 = np.percentile(latencies, 50)
        p95 = np.percentile(latencies, 95)
        mean_lat = np.mean(latencies)
        
        results.append({
            'batch': batch_size,
            'p50': p50,
            'p95': p95,
            'mean': mean_lat
        })
        
        print(f"  Batch {batch_size}: P50={p50:.2f}ms, P95={p95:.2f}ms, Mean={mean_lat:.2f}ms")
    
    return results

# TODO: Run the experiment
# Hint: benchmark_batch_size("./models/model.onnx", [1, 8, 32], runs=10)

print("‚úÖ Benchmark function ready")
```


## cell 10 [code]

```python
# Run the benchmark experiment
model_path = "./models/model.onnx"
if not os.path.exists(model_path):
    print("‚ùå Model not found. Please complete Notebook 01 first.")
    print("Expected path:", model_path)
else:
    # TODO: Run the benchmark with batch sizes [1, 8, 32]
    results = benchmark_batch_size(model_path, [1, 8, 32], runs=10)
    
    # Display results in a table
    print("\nüìä Benchmark Results:")
    print("Batch Size | P50 (ms) | P95 (ms) | Mean (ms)")
    print("-" * 45)
    for r in results:
        print(f"{r['batch']:10} | {r['p50']:8.2f} | {r['p95']:8.2f} | {r['mean']:8.2f}")
    
    # Auto-check
    assert len(results) >= 3 and all({'batch','p50','p95'} <= set(r) for r in results)
    print("‚úÖ Results format OK")
```


## cell 11 [code]

```python
hints(
    "Use matplotlib: hist + vertical lines at np.percentile(..., 50/95).",
    "Label axes: milliseconds; title with provider/batch size.",
    solution='''
import numpy as np, matplotlib.pyplot as plt

def plot_latency(lat_ms, title="Latency"):
    p50 = np.percentile(lat_ms, 50)
    p95 = np.percentile(lat_ms, 95)
    plt.figure()
    plt.hist(lat_ms, bins=20)
    plt.axvline(p50, linestyle="--", label=f"P50={p50:.2f} ms")
    plt.axvline(p95, linestyle="--", label=f"P95={p95:.2f} ms")
    plt.xlabel("Latency (ms)"); plt.ylabel("Count"); plt.title(title); plt.legend()
    plt.show()
'''
)

# Visualize the results
if 'results' in locals() and len(results) >= 3:
    batch_sizes = [r['batch'] for r in results]
    p50_values = [r['p50'] for r in results]
    p95_values = [r['p95'] for r in results]
    mean_values = [r['mean'] for r in results]
    
    plt.figure(figsize=(10, 6))
    plt.plot(batch_sizes, p50_values, 'o-', label='P50 Latency', linewidth=2)
    plt.plot(batch_sizes, p95_values, 's-', label='P95 Latency', linewidth=2)
    plt.plot(batch_sizes, mean_values, '^-', label='Mean Latency', linewidth=2)
    
    plt.xlabel('Batch Size')
    plt.ylabel('Latency (ms)')
    plt.title('Latency vs Batch Size')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print("üìà Chart shows latency trends across batch sizes")
else:
    print("‚ö†Ô∏è No results to visualize. Run the benchmark first.")
```


## cell 12 [markdown]

hints(
    "Larger batches can improve throughput but may increase single-sample latency.",
    "Edge scenarios often prioritize tail latency (P95) over throughput.",
    solution="""\
Batching amortizes overhead per call (‚Üë throughput), but single-request latency
often grows with batch size. On-device UX typically targets low P95, so choose
small batches unless you have parallel demand."""
)

## Analysis Questions

Based on your benchmark results, answer these questions:

**1. How does latency change as batch size increases? Explain the trend.**

*Your answer here (2-3 sentences):*

---

**2. Why might P95 latency be higher than P50 latency? What does this tell us about system performance?**

*Your answer here (2-3 sentences):*

---

**3. If you were deploying this model to a real-time application, which batch size would you choose and why?**

*Your answer here (2-3 sentences):*


## cell 13 [markdown]

## Next Steps

Excellent work! You've learned how to measure and analyze model performance.

**Next**: Open `03_quantization.ipynb` to learn about model compression and optimization.

---

### Summary
- ‚úÖ Understood latency vs throughput concepts
- ‚úÖ Implemented warm-up benchmarking
- ‚úÖ Analyzed batch size effects on performance
- ‚úÖ Interpreted P50/P95 latency metrics


## cell 14 [markdown]

# ‚ö° Latency Benchmark - Understand model performance

**Goal**: Understand how we measure and interpret model latency (response time).

In this notebook we will:
- Understand what latency is and why it's important
- See how benchmark works (warmup, runs, providers)
- Interpret results (p50, p95, histogram)
- Experiment with different settings

> **üí° Tip**: Latency is critical for edge deployment - a model that's too slow is not usable in real life!


## cell 15 [markdown]

## ü§î What is latency and why is it important?

**Latency** = the time it takes for the model to make a prediction (inference time).

**Why important for edge**:
- **Real-time applications** - robots, autonomous vehicles
- **User experience** - no one wants to wait 5 seconds for image classification
- **Resource constraints** - Raspberry Pi has limited CPU/memory

<details>
<summary>üîç Click to see typical latency targets</summary>

**Typical latency targets**:
- **< 10ms**: Real-time video, gaming
- **< 100ms**: Interactive applications
- **< 1000ms**: Batch processing, offline analysisisisisisis

**Our model**: Expect ~1-10ms on CPU (good for edge!)

</details>


## cell 16 [markdown]

## üîß How does benchmark work?

**Benchmark process**:
1. **Warmup** - run the model a few times to "warm up" (JIT compilation, cache)
2. **Runs** - measure latency for many runs
3. **Statistics** - calculate p50, p95, mean, std

**Why warmup?**
- First run is often slow (JIT compilation)
- Cache warming affects performance
- We want to measure "steady state" performance


## cell 17 [code]

```python
# Run benchmark with different settings
print("üöÄ Running benchmark...")

# Use the model from the previous notebook (or create a quick one)
!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_bench
```


## cell 18 [code]

```python
# Benchmark with different numbers of runs to see variance
import os

# Test 1: Few runs (fast)
print("üìä Test 1: 10 runs")
!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup 3 --runs 10 --providers CPUExecutionProvider
```


## cell 19 [code]

```python
# Show Benchmark results
if os.path.exists("./reports/latency_summary.txt"):
    with open("./reports/latency_summary.txt", "r") as f:
        print("üìà Benchmark results:")
        print(f.read())
else:
    print("‚ùå Benchmark report missing")
```


## cell 20 [code]

```python
# Read detailed latency data and visualize
import pandas as pd
import matplotlib.pyplot as plt

if os.path.exists("./reports/latency.csv"):
    df = pd.read_csv("./reports/latency.csv")
    
    print(f"üìä Latency statistics:")
    print(f"Num measurements: {len(df)}")
    print(f"Mean: {df['latency_ms'].mean():.2f} ms")
    print(f"Std: {df['latency_ms'].std():.2f} ms")
    print(f"Min: {df['latency_ms'].min():.2f} ms")
    print(f"Max: {df['latency_ms'].max():.2f} ms")
    
    # Histogram
    plt.figure(figsize=(10, 6))
    plt.hist(df['latency_ms'], bins=20, alpha=0.7, edgecolor='black')
    plt.xlabel('Latency (ms)')
    plt.ylabel('Count')
    plt.title('Latency distribution')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # Box plot
    plt.figure(figsize=(8, 6))
    plt.boxplot(df['latency_ms'])
    plt.ylabel('Latency (ms)')
    plt.title('Latency Box Plot')
    plt.grid(True, alpha=0.3)
    plt.show()
else:
    print("‚ùå Latency CSV missing")
```


## cell 21 [markdown]

## ü§î Reflection Questions

<details>
<summary>üí≠ Why is p95 more important than mean for edge deployment?</summary>

**Answer**: p95 (95th percentile) shows the worst latency that 95% of users experience. It is more important than mean because:

- **User experience**: A user who gets 100ms latency will notice it, even if the mean is 10ms
- **SLA targets**: Many systems have SLA targets at p95 latency
- **Outliers**: Mean can be skewed by outliers; p95 is more robust

</details>

<details>
<summary>üí≠ What happens to latency variance when you increase the number of runs?</summary>

**Answer**: With more runs we get:
- **More stable statistics** - p50/p95 become more reliable
- **Better understanding of variance** - see if the model is consistent
- **Less impact of outliers** - occasional slow runs matter less

**Experiment**: Run the benchmark with 10, 50, 100 runs and compare standard deviation.

</details>


## cell 22 [markdown]

## üéØ Your own experiment

**Task**: Run the benchmark with different settings and compare the results.

**Suggestions**:
- Try different numbers of runs (10, 50, 100)
- Compare the warmup effect (0, 3, 10 warmup)
- Analyze the variance between runs

**Code to modify**:
```python
# Change these values:
WARMUP_RUNS = 5
BENCHMARK_RUNS = 50

!python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider
```


## cell 23 [code]

```python
# TODO: Implement your experiment here
# Change the values below and run the benchmark

WARMUP_RUNS = 5
BENCHMARK_RUNS = 50

print(f"üß™ My experiment: warmup={WARMUP_RUNS}, runs={BENCHMARK_RUNS}")

# TODO: Run the benchmark with your settings
# !python -m piedge_edukit.benchmark --fakedata --model-path ./models_bench/model.onnx --warmup {WARMUP_RUNS} --runs {BENCHMARK_RUNS} --providers CPUExecutionProvider
```


## cell 24 [markdown]

## üéâ Summary

You have now learned:
- What latency is and why it is critical for edge deployment
- How the benchmark works (warmup, runs, statistics)
- How to interpret latency results (p50, p95, variance)
- Why P95 is more important than mean for user experience

**Next step**: Go to `03_quantization.ipynb` to understand how quantization can improve performance.

**Key concepts**:
- **Latency**: Inference time (critical for edge)
- **Warm-up**: Prepares the model for measurement
- **p50/p95**: Percentiles for the latency distribution
- **Variance**: Consistency in performance


# ==== notebooks\03_quantization.ipynb ====



## cell 1 [code]

```python
# Bootstrap: Import helpers and create directories
import sys
from pathlib import Path

# Add repo root to Python path
repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from utils.nb_helpers import run_module, run_script
print("‚úÖ Notebook helpers loaded - ready for quantization!")
```


## cell 2 [markdown]

# 03 - Model Quantization & Compression (INT8)

## Learning Goals

* Understand **post-training quantization** (PTQ): convert float (FP32) weights/activations to **INT8**.
* Know the role of **calibration data** and why it must match training preprocessing.
* Compare FP32 vs INT8 latency and size; understand when PTQ may fail.

## You Should Be Able To...

- Explain why quantization is useful for edge deployment
- Run quantization experiments with different calibration sizes
- Compare FP32 vs INT8 model performance and size
- Identify when quantization fails and why
- Make informed decisions about quantization for deployment

---

## Concepts

**Static quantization**: needs calibration samples to estimate activation ranges (min/max).

**Calibration**: run several inputs through the (float) model to collect statistics; the choice of data strongly affects quality.

**Failure modes**: unsupported ops, numerical sensitivity, or runtime limitations. Fallback to FP32 is OK in a demo.

**Trade-offs**: INT8 reduces model size and often speeds up CPU inference; may degrade accuracy if the model is sensitive.

## Best Practice

* Use a handful (e.g., 32‚Äì128) **representative** images with the **same preprocessing** as training.
* Document your preprocessing and keep it consistent across train/quantize/deploy.

## Common Pitfalls

* Using different preprocessing for calibration vs training data
* Expecting quantization to always succeed (some ops/runtimes don't support INT8)
* Not measuring accuracy after quantization
* Using too few calibration samples for representative statistics

## Success Criteria

* ‚úÖ Quantization step either succeeds **or** fails cleanly with a clear message
* ‚úÖ Summary compares FP32 vs INT8 size/latency
* ‚úÖ You can articulate the trade-off and when you would choose INT8

## Reflection

After completing this notebook, reflect on:
- When would you choose INT8 over FP32 for deployment?
- What factors determine if quantization will succeed?
- How does calibration data quality affect quantization results?

---

## Setup & Environment Check


## cell 3 [code]

```python
# ruff: noqa: E401
import os
import sys
from pathlib import Path

if Path.cwd().name == "labs":
    os.chdir(Path.cwd().parent)
    print("‚Üí Working dir set to repo root:", os.getcwd())
if os.getcwd() not in sys.path:
    sys.path.insert(0, os.getcwd())

import time
import numpy as np
import matplotlib.pyplot as plt
import onnxruntime as ort
from piedge_edukit.preprocess import FakeData as PEDFakeData
import piedge_edukit as _pkg  # noqa: F401

# Hints & Solutions helper (pure Jupyter, no extra deps)
from IPython.display import Markdown, display

def hints(*lines, solution: str | None = None, title="Need a nudge?"):
    """Render progressive hints + optional collapsible solution."""
    md = [f"### {title}"]
    for i, txt in enumerate(lines, start=1):
        md.append(f"<details><summary>Hint {i}</summary>\n\n{txt}\n\n</details>")
    if solution:
        # keep code fenced as python for readability
        md.append(
            "<details><summary><b>Show solution</b></summary>\n\n"
            f"```python\n{solution.strip()}\n```\n"
            "</details>"
        )
    display(Markdown("\n\n".join(md)))
```


## cell 4 [code]

```python
# Environment self-heal (Python 3.12 + editable install)
import subprocess
import importlib

print(f"Python: {sys.version.split()[0]} (need 3.12)")

try:
    import piedge_edukit  # noqa: F401
    print("‚úÖ PiEdge EduKit package OK")
except ModuleNotFoundError:
    print("‚ÑπÔ∏è Installing package in editable mode ‚Ä¶")
    root = os.getcwd()
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-e", root])
    importlib.invalidate_caches()
    import piedge_edukit  # noqa: F401
    print("‚úÖ Package installed")
```


## cell 5 [code]

```python
# All imports are now in the first cell above
print("‚úÖ All imports successful")
```


## cell 6 [markdown]

## Concept: Quantization

**Quantization** reduces model precision to improve performance:

- **FP32**: 32-bit floating point (default PyTorch precision)
- **INT8**: 8-bit integer (4x smaller, often 2-4x faster)

**Benefits**:
- Smaller model size (important for mobile/edge)
- Faster inference (less memory bandwidth)
- Lower power consumption

**Trade-offs**:
- Potential accuracy loss
- Some operations may not be quantizable
- Calibration data required for optimal scaling


## cell 7 [markdown]

## Task A: Calibration Size Experiment

Test quantization with different calibration dataset sizes and compare results.

**Note**: On some environments, INT8 quantization may not be supported. In such cases, we'll show FP32 baseline and mark fallback in the summary - this is acceptable for this lesson.

### TODO A1 ‚Äî Run static PTQ with different calib sizes
Try `--calib-size 16` and `--calib-size 64`, compare latency/accuracy.

<details><summary>Hint 1</summary>
Re-use the *same preprocessing* as FP32 for calibration data.
</details>

<details><summary>Solution</summary>

```bash
python -m piedge_edukit.quantization --data-path data/train --calib-size 16
python -m piedge_edukit.quantization --data-path data/train --calib-size 64
```

Record results in `reports/quantization_comparison.csv`.

</details>


## cell 8 [code]

```python
# TODO A1: launch two PTQ runs (16 and 64) and append results to reports/quantization_comparison.csv

def run_quantization_experiment(model_path, calib_sizes):
    """
    Run quantization experiments with different calibration sizes.
    Returns summary with fp32_ms, int8_ms, fp32_mb, int8_mb (if available).
    """
    summary = {}
    
    # Load FP32 model for baseline
    fp32_session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])
    fp32_size_mb = os.path.getsize(model_path) / (1024*1024)
    summary['fp32_mb'] = fp32_size_mb
    
    # Benchmark FP32 latency
    input_name = fp32_session.get_inputs()[0].name
    output_name = fp32_session.get_outputs()[0].name
    
    # Warm-up
    dummy_input = np.random.randn(1, 3, 64, 64).astype(np.float32)
    for _ in range(3):
        _ = fp32_session.run([output_name], {input_name: dummy_input})
    
    # Measure FP32 latency
    import time
    latencies = []
    for _ in range(10):
        start = time.time()
        _ = fp32_session.run([output_name], {input_name: dummy_input})
        end = time.time()
        latencies.append((end - start) * 1000)
    
    summary['fp32_ms'] = np.mean(latencies)
    
    print(f"FP32 baseline: {summary['fp32_ms']:.2f}ms, {summary['fp32_mb']:.2f}MB")
    
    # Try quantization for each calibration size
    int8_results = []
    for calib_size in calib_sizes:
        print(f"\\nTrying calibration size {calib_size}...")
        try:
            # This is a simplified quantization attempt
            # In practice, you'd use proper quantization tools
            print(f"  Calibration size {calib_size}: Quantization may fail on this platform")
            print(f"  This is normal - FP32 fallback is acceptable for this lesson")
            
        except Exception as e:
            print(f"  Quantization failed: {str(e)[:100]}...")
            print(f"  Continuing with FP32 only (acceptable for this lesson)")
    
    # Mark INT8 as unavailable
    summary['int8_ms'] = None
    summary['int8_mb'] = None
    summary['quantization_status'] = 'failed_fallback'
    
    return summary

# TODO: Run the experiment
# Hint: run_quantization_experiment("./models/model.onnx", [8, 32, 128])

print("‚úÖ Quantization experiment function ready")
```


## cell 9 [code]

```python
hints(
    "Call the CLI module: `python -m piedge_edukit.quantization --data-path data/train --calib-size 32`.",
    "Accept that INT8 can fail; fallback to FP32 is okay for the lesson.",
    solution='''
import sys, subprocess
args = [sys.executable, "-m", "piedge_edukit.quantization",
        "--data-path", "data/train",
        "--calib-size", "32",
        "--model-path", "models/model.onnx"]
subprocess.check_call(args)
'''
)

# Run the quantization experiment
model_path = "./models/model.onnx"
if not os.path.exists(model_path):
    print("‚ùå Model not found. Please complete Notebook 01 first.")
    print("Expected path:", model_path)
else:
    # TODO: Run the experiment with calibration sizes [8, 32, 128]
    summary = run_quantization_experiment(model_path, [8, 32, 128])
    
    # Display results
    print("\\nüìä Quantization Results:")
    print(f"FP32 Latency: {summary['fp32_ms']:.2f} ms")
    print(f"FP32 Size: {summary['fp32_mb']:.2f} MB")
    
    if summary['int8_ms'] is not None:
        print(f"INT8 Latency: {summary['int8_ms']:.2f} ms")
        print(f"INT8 Size: {summary['int8_mb']:.2f} MB")
        speedup = summary['fp32_ms'] / summary['int8_ms']
        size_reduction = (1 - summary['int8_mb'] / summary['fp32_mb']) * 100
        print(f"Speedup: {speedup:.2f}x")
        print(f"Size reduction: {size_reduction:.1f}%")
    else:
        print("INT8: Not available (quantization failed)")
        print("Status: FP32 fallback (acceptable for this lesson)")
    
    # Auto-check
    assert 'fp32_ms' in summary and 'fp32_mb' in summary
    print("‚úÖ Summary present (INT8 may be unavailable on this platform)")
```


## cell 10 [markdown]

hints(
    "Some ops or runtime combos don't quantize well; accuracy/latency may regress.",
    "Pick nodes/EPs carefully, and always measure after quantizing.",
    solution="""\
INT8 may fail for certain ops/EPs or degrade accuracy when calibration is weak.
Best practice: quantize with representative data, then measure latency & quality;
fallback to FP32 when INT8 brings no benefit."""
)

## Analysis Questions

Based on your quantization experiment, answer these questions:

**1. If INT8 quantization failed: what does the error suggest, and what would you try next on a different machine/provider?**

*Your answer here (2-3 sentences):*

---

**2. What are the main trade-offs between FP32 and INT8 precision?**

*Your answer here (2-3 sentences):*

---

**3. Why might quantization fail on some platforms but work on others?**

*Your answer here (2-3 sentences):*


## cell 11 [markdown]

## Next Steps

Great work! You've learned about model quantization and compression techniques.

**Next**: Open `04_evaluate_and_verify.ipynb` to complete the lesson with evaluation and verification.

---

### Summary
- ‚úÖ Understood FP32 vs INT8 precision trade-offs
- ‚úÖ Experimented with different calibration sizes
- ‚úÖ Analyzed quantization success/failure modes
- ‚úÖ Learned about fallback strategies


## cell 12 [markdown]

# ‚ö° Quantization (INT8) - Compress the model for faster inference

**Goal**: Understand how quantization works and when it is worth it.

In this notebook we will:
- Understand what quantization is (FP32 ‚Üí INT8)
- See how it affects model size and latency
- Experiment with different calibration sizes
- Understand the trade-offs (accuracy vs performance)

> **üí° Tips**: Quantization is one of the most important techniques for edge deployment - it can make the model 4x faster!


## cell 13 [markdown]

## ü§î What is quantization?

**Quantization** = convert the model from 32-bit floating point (FP32) to 8-bit integers (INT8).

**Benefits**:
- **4x smaller model size** (32-bit ‚Üí 8-bit)
- **2‚Äì4x faster inference** (INT8 is faster to compute)
- **Lower memory usage** (important for edge)

**Trade-offs**:
- **Accuracy loss** ‚Äî the model can become less accurate
- **Calibration required** ‚Äî needs representative data to find proper scales

<details>
<summary>üîç Click to see technical details</summary>

**Technical details**:
- FP32: 32 bits per weight (4 bytes)
- INT8: 8 bits per weight (1 byte)
- Quantization finds the right scale for each weight
- Calibration uses representative data to optimize scales

</details>


## cell 14 [code]

```python
# First create a model to quantize
print("üöÄ Creating a model for quantization...")
!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_quant
```


## cell 15 [code]

```python
# Check original model size
import os

if os.path.exists("./models_quant/model.onnx"):
    original_size = os.path.getsize("./models_quant/model.onnx") / (1024*1024)
    print(f"üì¶ Original model size: {original_size:.2f} MB")
else:
    print("‚ùå Model missing")
```


## cell 16 [markdown]

## üß™ Experiment with different calibration sizes

**Calibration size** = number of images used to find the right quantization scales.

**Larger calibration**:
- ‚úÖ Better accuracy (more representative data)
- ‚ùå Longer quantization time
- ‚ùå More memory during quantization

**Smaller calibration**:
- ‚úÖ Faster quantization
- ‚úÖ Lower memory usage
- ‚ùå Potentially worse accuracy


## cell 17 [code]

```python
# Test 1: Small calibration (fast)
print("‚ö° Test 1: Small calibration (16 images)")
!python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size 16
```


## cell 18 [code]

```python
# Show quantization results
if os.path.exists("./reports/quantization_summary.txt"):
    with open("./reports/quantization_summary.txt", "r") as f:
        print("üìä Quantization results:")
        print(f.read())
else:
    print("‚ùå Quantization report missing")
```


## cell 19 [code]

```python
# Compare model sizes
if os.path.exists("./models_quant/model.onnx") and os.path.exists("./models_quant/model_static.onnx"):
    original_size = os.path.getsize("./models_quant/model.onnx") / (1024*1024)
    quantized_size = os.path.getsize("./models_quant/model_static.onnx") / (1024*1024)
    
    print(f"üì¶ Model sizes:")
    print(f"  Original (FP32): {original_size:.2f} MB")
    print(f"  Quantized (INT8): {quantized_size:.2f} MB")
    print(f"  Compression: {original_size/quantized_size:.1f}x")
else:
    print("‚ùå Model files missing")
```


## cell 20 [code]

```python
# Benchmark both models to compare latency
print("üöÄ Benchmark original model (FP32)...")
!python -m piedge_edukit.benchmark --fakedata --model-path ./models_quant/model.onnx --warmup 3 --runs 20 --providers CPUExecutionProvider
```


## cell 21 [code]

```python
# Benchmark quantized model (INT8)
print("‚ö° Benchmark quantized model (INT8)...")
!python -m piedge_edukit.benchmark --fakedata --model-path ./models_quant/model_static.onnx --warmup 3 --runs 20 --providers CPUExecutionProvider
```


## cell 22 [code]

```python
# Compare latency results
import pandas as pd

# Read both benchmark results
fp32_file = "./reports/latency_summary.txt"
if os.path.exists(fp32_file):
    with open(fp32_file, "r") as f:
        fp32_content = f.read()
    
    # Extract mean latency from the text (simple parsing)
    lines = fp32_content.split('\n')
    fp32_mean = None
    for line in lines:
        if 'Mean' in line and 'ms' in line:
            try:
                fp32_mean = float(line.split(':')[1].strip().replace('ms', '').strip())
                break
            except:
                pass
    
    print(f"üìä Latency comparison:")
    if fp32_mean:
        print(f"  FP32 (original): {fp32_mean:.2f} ms")
    else:
        print(f"  FP32: Could not parse latency")
    
    # TODO: Add INT8 latency here when available
    print(f"  INT8 (quantized): [after benchmark]")
else:
    print("‚ùå Benchmark report missing")
```


## cell 23 [markdown]

## ü§î Reflection Questions

<details>
<summary>üí≠ When is INT8 quantization worth it?</summary>

**Answer**: INT8 is worth it when:
- **Latency is critical** ‚Äî real-time applications, edge deployment
- **Memory is limited** ‚Äî mobile, Raspberry Pi
- **Accuracy loss is acceptable** ‚Äî < 1‚Äì2% accuracy drop is often OK
- **Batch size is small** ‚Äî quantization often works best with small batches

**When NOT worth it**:
- Accuracy is absolutely critical
- You have ample memory and CPU
- The model is already fast enough

</details>

<details>
<summary>üí≠ What are the risks with quantization?</summary>

**Answer**: Main risks:
- **Accuracy loss** ‚Äî the model can become less accurate
- **Calibration data** ‚Äî needs representative data for good quantization
- **Edge cases** ‚Äî extreme values can cause issues
- **Debugging** ‚Äî quantized models are harder to debug

**Mitigation**:
- Test thoroughly with real data
- Use different calibration sizes
- Benchmark both accuracy and latency

</details>


## cell 24 [markdown]

## üéØ Your own experiment

**Task**: Test different calibration sizes and compare the results.

**Suggestions**:
- Try calibration sizes: 8, 16, 32, 64
- Compare model size and latency
- Analyze accuracy loss (if available)

**Code to modify**:
```python
# Change these values:
CALIB_SIZE = 32

!python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size {CALIB_SIZE}
```


## cell 25 [code]

```python
# TODO: Implement your experiment here
# Change the values below and run quantization

CALIB_SIZE = 32

print(f"üß™ My experiment: calibration_size={CALIB_SIZE}")

# TODO: Run quantization with your setting
# !python -m piedge_edukit.quantization --fakedata --model-path ./models_quant/model.onnx --calib-size {CALIB_SIZE}
```


## cell 26 [markdown]

## üéâ Summary

You have now learned:
- What quantization is (FP32 ‚Üí INT8) and why it matters
- How calibration size affects the result
- Trade-offs between accuracy and performance
- When quantization is worth it vs when it is not

**Next**: Open `04_evaluate_and_verify.ipynb` to understand automated checks and receipt generation.

**Key concepts**:
- **Quantization**: FP32 ‚Üí INT8 for faster inference
- **Calibration**: Representative data to find the right scales
- **Compression**: 4x smaller model size
- **Speedup**: 2‚Äì4x faster inference


# ==== notebooks\04_evaluate_and_verify.ipynb ====



## cell 1 [code]

```python
# Bootstrap: Import helpers and create directories
import sys
from pathlib import Path

# Add repo root to Python path
repo_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))

from utils.nb_helpers import run_module, run_script
print("‚úÖ Notebook helpers loaded - ready for evaluation!")
```


## cell 2 [markdown]

# 04 - Evaluation & Verification

## Learning Goals

* Compute and interpret simple metrics (accuracy, confusion matrix).
* Produce reproducible **artifacts** and a machine-readable **receipt** for CI.
* Understand why verifiable outputs matter in a classroom or production pipeline.

## You Should Be Able To...

- Run model evaluation and interpret results
- Understand confusion matrices and accuracy metrics
- Generate verification receipts for ML pipelines
- Identify when models meet deployment criteria
- Reflect on the complete ML development process

---

## Concepts

**Confusion matrix**: where the classifier makes mistakes by class.

**Reproducible artifacts**: model file, benchmark reports, quantization summary, evaluation report.

**Receipt**: a small JSON proving all required files were created and basic checks passed.

## Common Pitfalls

* Not running evaluation on held-out test data
* Misinterpreting confusion matrix results
* Forgetting to generate verification artifacts
* Not checking that all pipeline components work together

## Success Criteria

* ‚úÖ `progress/receipt.json` says **PASS**
* ‚úÖ You can explain what each artifact is and where it lives
* ‚úÖ You can describe one change you'd make next (e.g., more data, different architecture)

---

## Setup & Environment Check


## cell 3 [markdown]

# ruff: noqa: E401
import os
import sys
from pathlib import Path

def cd_repo_root():
    p = Path.cwd()
    for _ in range(5):  # climb up at most 5 levels
        if (p/"verify.py").exists() and (p/"scripts"/"evaluate_onnx.py").exists():
            if str(p) not in sys.path: sys.path.insert(0, str(p))
            if p != Path.cwd():
                os.chdir(p)
                print("-> Changed working dir to repo root:", os.getcwd())
            return
        p = p.parent
    raise RuntimeError("Could not locate repo root")

cd_repo_root()

# Hints & Solutions helper (pure Jupyter, no extra deps)
from IPython.display import Markdown, display

def hints(*lines, solution: str | None = None, title="Need a nudge?"):
    """Render progressive hints + optional collapsible solution."""
    md = [f"### {title}"]
    for i, txt in enumerate(lines, start=1):
        md.append(f"<details><summary>Hint {i}</summary>\n\n{txt}\n\n</details>")
    if solution:
        # keep code fenced as python for readability
        md.append(
            "<details><summary><b>Show solution</b></summary>\n\n"
            f"```python\n{solution.strip()}\n```\n"
            "</details>"
        )
    display(Markdown("\n\n".join(md)))


## cell 4 [markdown]

## ü§î What is evaluation and why do we need it?

**Evaluation** = test the model on data it has not seen during training.

**What we measure**:
- **Accuracy** ‚Äî how many predictions are correct
- **Confusion matrix** ‚Äî detailed breakdown of correct/incorrect predictions
- **Per-class performance** ‚Äî how well the model performs for each class

**Why important**:
- **Validation** ‚Äî ensures the model actually works
- **Debugging** ‚Äî shows which classes are difficult
- **Comparison** ‚Äî compare different models/settings

<details>
<summary>üîç Click to see what a confusion matrix shows</summary>

**Confusion matrix**:
- **Diagonal** = correct predictions
- **Off-diagonal** = incorrect predictions
- **Per class** = precision, recall for each class

</details>


## cell 5 [code]

```python
# Run evaluation on our model
print("üîç Running evaluation...")

# Use the model from previous notebooks (or create a quick one)
!python -m piedge_edukit.train --fakedata --no-pretrained --epochs 1 --batch-size 256 --output-dir ./models_eval
```


## cell 6 [code]

```python
# Run evaluation with a limited number of samples (faster)
!python scripts/evaluate_onnx.py --model ./models_eval/model.onnx --fakedata --limit 32
```


## cell 7 [code]

```python
# Show evaluation results
import os

if os.path.exists("./reports/eval_summary.txt"):
    with open("./reports/eval_summary.txt", "r") as f:
        print("üìä Evaluation results:")
        print(f.read())
else:
    print("‚ùå Evaluation report missing")
```


## cell 8 [code]

```python
# Show training curves if available
from PIL import Image
from IPython.display import display

if os.path.exists("./reports/training_curves.png"):
    print("üìà Training curves:")
    display(Image.open("./reports/training_curves.png"))
else:
    print("‚ö†Ô∏è Training curves missing ‚Äì run training first.")
```


## cell 10 [code]

```python
# Show confusion matrix om den finns
import matplotlib.pyplot as plt
from PIL import Image

if os.path.exists("./reports/confusion_matrix.png"):
    print("üìà Confusion Matrix:")
    img = Image.open("./reports/confusion_matrix.png")
    plt.figure(figsize=(8, 6))
    plt.imshow(img)
    plt.axis('off')
    plt.title('Confusion Matrix')
    plt.show()
else:
    print("‚ùå Confusion matrix missing")
```


## cell 11 [markdown]

## üîç Automatic verification

**Verification** = automated checks ensuring the lesson works correctly.

**What is checked**:
- **Artifacts exist** ‚Äî all required files are created
- **Benchmark works** ‚Äî latency data is valid
- **Quantization works** ‚Äî quantized model is created
- **Evaluation works** ‚Äî confusion matrix and accuracy are available

**Result**: `progress/receipt.json` with PASS/FAIL status


## cell 12 [code]

```python
# Run automatic verification
print("üîç Running automatic verification...")
!python verify.py
```


## cell 13 [code]

```python
# Analyze the receipt in detail
import json

if os.path.exists("./progress/receipt.json"):
    with open("./progress/receipt.json", "r") as f:
        receipt = json.load(f)
    
    print("üìã Detailed receipt analysis:")
    print(f"Status: {'‚úÖ PASS' if receipt['pass'] else '‚ùå FAIL'}")
    print(f"Timestamp: {receipt['timestamp']}")
    
    print("\nüîç Checks:")
    for check in receipt['checks']:
        status = "‚úÖ" if check['ok'] else "‚ùå"
        print(f"  {status} {check['name']}: {check['reason']}")
    
    print("\nüìä Metrics:")
    if 'metrics' in receipt:
        for metric, value in receipt['metrics'].items():
            print(f"  {metric}: {value}")
    
    print("\nüìÅ Generated files:")
    if 'artifacts' in receipt:
        for artifact in receipt['artifacts']:
            print(f"  - {artifact}")
else:
    print("‚ùå Receipt missing")
```


## cell 14 [markdown]

## ü§î Reflection Questions

### TODO R1 ‚Äî Reflect on results (2‚Äì4 bullets)
- Where did quantization help / hurt?
- Do your p50 and p95 match expectations after warm-up?
- One change you would make before deploying.

<details><summary>Hint</summary>
Tie back to goals: correctness, latency, and determinism. Fallback to FP32 is fine if INT8 regresses.
</details>

<details>
<summary>üí≠ Which goals are verified by our automatic check?</summary>

**Answer**: Our verification checks:
- **Technical functionality** ‚Äî all steps run without errors
- **Artifact generation** ‚Äî required files are created
- **Data integrity** ‚Äî reports are valid and parseable
- **Pipeline integration** ‚Äî all components work together

**What is NOT verified**:
- Accuracy quality (only that evaluation runs)
- Latency targets (only that benchmark runs)
- Production readiness (only that the pipeline works)

</details>

<details>
<summary>üí≠ What is missing for "production"?</summary>

**Answer**: For production we need:
- **Real data** ‚Äî not FakeData
- **Accuracy targets** ‚Äî specific precision/recall requirements
- **Latency targets** ‚Äî SLA requirements on inference time
- **Robustness** ‚Äî handling of edge cases and errors
- **Monitoring** ‚Äî continuous monitoring of performance
- **A/B testing** ‚Äî comparison of different models
- **Rollback** ‚Äî ability to revert to previous versions

</details>


## cell 15 [markdown]

## üéØ Your own experiment

**Task**: Run verification on different models and compare receipts.

**Suggestions**:
- Train models with different settings
- Run verification on each model
- Compare receipts and see which pass/fail
- Analyze which checks are most critical

**Code to modify**:
```python
# Train different models and run verification
MODELS = [
    {"epochs": 1, "batch_size": 128, "name": "quick"},
    {"epochs": 3, "batch_size": 64, "name": "balanced"},
    {"epochs": 5, "batch_size": 32, "name": "thorough"}
]

for model_config in MODELS:
    # Train model
    # Run verification
    # Analyze the receipt
```


## cell 16 [code]

```python
# TODO: Implement your experiment here
# Train different models and compare the receipts

MODELS = [
    {"epochs": 1, "batch_size": 128, "name": "quick"},
    {"epochs": 3, "batch_size": 64, "name": "balanced"},
    {"epochs": 5, "batch_size": 32, "name": "thorough"}
]

print("üß™ My experiment: Compare different models")
for model_config in MODELS:
    print(f"  - {model_config['name']}: epochs={model_config['epochs']}, batch_size={model_config['batch_size']}")

# TODO: Implement a loop that trains and verifies each model
```


## cell 17 [markdown]

## Final Reflection

Congratulations! You've completed the entire PiEdge EduKit lesson. Please reflect on your learning experience:

**1. What was the most challenging part of implementing the CNN architecture? What helped you understand it better?**

*Your answer here (2-3 sentences):*

---

**2. How did your understanding of model performance change after running the latency benchmarks?**

*Your answer here (2-3 sentences):*

---

**3. What surprised you most about the quantization process? What would you do differently in a real deployment?**

*Your answer here (2-3 sentences):*

---

**4. How important do you think automated verification is for ML pipelines? Why?**

*Your answer here (2-3 sentences):*

---

## Next Steps

**Congratulations!** You've successfully completed the PiEdge EduKit lesson. You now understand:

- ‚úÖ CNN implementation and training
- ‚úÖ Model export to ONNX format  
- ‚úÖ Performance benchmarking and analysis
- ‚úÖ Quantization and compression techniques
- ‚úÖ Evaluation and verification workflows

**Real-world applications**: Experiment with real data, different models, or deploy on Raspberry Pi!

**Key concepts mastered**:
- **Training**: Implementing and training neural networks
- **Export**: Converting models to deployment-ready formats
- **Benchmarking**: Measuring and analyzing performance
- **Quantization**: Optimizing models for edge deployment
- **Verification**: Automated quality assurance for ML pipelines
